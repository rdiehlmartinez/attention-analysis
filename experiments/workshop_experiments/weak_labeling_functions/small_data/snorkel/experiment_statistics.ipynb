{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snorkel Time! - Evaluating predictions\n",
    "------\n",
    "In this experiment we simply evaluate the accuracy of the snorkel labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append(\"../../../../..\")\n",
    "import torch \n",
    "from src.experiment import ClassificationExperiment\n",
    "from src.dataset import ExperimentDataset\n",
    "from src.params import Params\n",
    "from metal.label_model import LabelModel # for labeling\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params.read_params(\"experiment_params.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/06/2020 22:33:35 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "285it [00:00, 3516.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading in the dataset that we are using in this experiments \n",
    "# typically this dataset is the small set of ground-truth labels\n",
    "train_dataset = ExperimentDataset.init_dataset(params.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/06/2020 22:33:31 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "100it [00:00, 2915.46it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_dataset = ExperimentDataset.init_dataset(params.dataset, data_path=params.dataset['labeled_data_dev'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Featurizers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Marta Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the Featurizer created by Pryzant et al.\n",
    "from src.utils.weak_labeling_utils import get_marta_featurizer, extract_marta_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/06/2020 22:42:00 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "featurizer = get_marta_featurizer(params.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189c4d22f11f43c198ef20d84c0cd290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=242.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "marta_features_train = extract_marta_features(train_dataset, featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77bfe707043f4e329326f81e9f30485a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=81.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "marta_features_dev = extract_marta_features(dev_dataset, featurizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT Embedding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.weak_labeling_utils import get_bert_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/06/2020 22:42:05 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /sailhome/rdm/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "03/06/2020 22:42:05 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /sailhome/rdm/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpssynxkho\n",
      "03/06/2020 22:42:09 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71914337fa343e1a7583d5caa114ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=242.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_embeddings_train = get_bert_features(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/06/2020 22:44:45 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /sailhome/rdm/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "03/06/2020 22:44:45 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /sailhome/rdm/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmparczvwe4\n",
      "03/06/2020 22:44:48 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7927150c2dda41399589592e7fb663d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=81.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_embeddings_dev = get_bert_features(dev_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.weak_labeling_utils import get_pos_features_multi_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/06/2020 22:53:28 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc85ed00bed4173a343073959136f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=242.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe00185eb854fb0b366b85ed11b50c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=81.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pos_matrices = get_pos_features_multi_dataset(params.dataset, [train_dataset, dev_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_features_train = pos_matrices[0]\n",
    "pos_features_dev = pos_matrices[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snorkel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create predictions for all of our weak labeling functions which we then combine together. In creating predictions, we run 200 epochs on the training set and get our predicted labels for the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.add_data(pos_features_train, \"pos_features\")\n",
    "train_dataset.add_data(marta_features_train, \"marta_features\")\n",
    "train_dataset.add_data(bert_embeddings_train, \"bert_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_dataloader = train_dataset.return_dataloader(batch_size = params.final_task['training_params']['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset.add_data(pos_features_dev, \"pos_features\")\n",
    "dev_dataset.add_data(marta_features_dev, \"marta_features\")\n",
    "dev_dataset.add_data(bert_embeddings_dev, \"bert_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset_dataloader = dev_dataset.return_dataloader(batch_size = params.final_task['training_params']['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting POS predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos_tags = pos_features_train.shape[1]\n",
    "params.final_task['input_dim'] = num_pos_tags\n",
    "params.final_task['hidden_dim'] = num_pos_tags//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_experiment_pos = ClassificationExperiment.init_cls_experiment(params.final_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epochs', max=200.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pos_all_losses, pos_all_evaluations = classification_experiment_pos.train_model(train_dataloader=train_dataset_dataloader,\n",
    "                                          eval_dataloader=train_dataset_dataloader,\n",
    "                                          input_key=\"pos_features\",\n",
    "                                          label_key=\"bias_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_predictions, _ = classification_experiment_pos.run_inference(dev_dataset_dataloader,\n",
    "                                                                 input_key=\"pos_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Marta Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "marta_features_size = marta_features.shape[1]\n",
    "params.final_task['input_dim'] = marta_features_size\n",
    "params.final_task['hidden_dim'] = marta_features_size//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_experiment_marta = ClassificationExperiment.init_cls_experiment(params.final_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epochs', max=200.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "marta_all_losses, marta_all_evaluations = classification_experiment_marta.train_model(train_dataloader=train_dataset_dataloader,\n",
    "                                          eval_dataloader=train_dataset_dataloader,\n",
    "                                          input_key=\"marta_features\",\n",
    "                                          label_key=\"bias_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "marta_predictions, _ = classification_experiment_marta.run_inference(dev_dataset_dataloader,\n",
    "                                                                 input_key=\"marta_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting BERT Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embedding_size = bert_embeddings.shape[1]\n",
    "params.final_task['input_dim'] = bert_embedding_size\n",
    "params.final_task['hidden_dim'] = bert_embedding_size//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_experiment_bert = ClassificationExperiment.init_cls_experiment(params.final_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epochs', max=200.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_all_losses, bert_all_evaluations = classification_experiment_bert.train_model(train_dataloader=train_dataset_dataloader,\n",
    "                                          eval_dataloader=train_dataset_dataloader,\n",
    "                                          input_key=\"bert_embeddings\",\n",
    "                                          label_key=\"bias_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_predictions, _ = classification_experiment_bert.run_inference(dev_dataset_dataloader,\n",
    "                                                                 input_key=\"bert_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Snorkel labeling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = [pos_predictions, marta_predictions, bert_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.weak_labeling_utils import generate_snorkel_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_matrix = generate_snorkel_matrix(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 3)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf_matrix.shape #checking that the dimensions are all as expeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_model = LabelModel(k=2, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing O...\n",
      "Estimating \\mu...\n",
      "[1 epo]: TRAIN:[loss=1.567]\n",
      "[2 epo]: TRAIN:[loss=1.505]\n",
      "[3 epo]: TRAIN:[loss=1.388]\n",
      "[4 epo]: TRAIN:[loss=1.226]\n",
      "[5 epo]: TRAIN:[loss=1.031]\n",
      "[6 epo]: TRAIN:[loss=0.816]\n",
      "[7 epo]: TRAIN:[loss=0.600]\n",
      "[8 epo]: TRAIN:[loss=0.403]\n",
      "[9 epo]: TRAIN:[loss=0.245]\n",
      "[10 epo]: TRAIN:[loss=0.143]\n",
      "[11 epo]: TRAIN:[loss=0.105]\n",
      "[12 epo]: TRAIN:[loss=0.125]\n",
      "[13 epo]: TRAIN:[loss=0.186]\n",
      "[14 epo]: TRAIN:[loss=0.259]\n",
      "[15 epo]: TRAIN:[loss=0.316]\n",
      "[16 epo]: TRAIN:[loss=0.336]\n",
      "[17 epo]: TRAIN:[loss=0.315]\n",
      "[18 epo]: TRAIN:[loss=0.259]\n",
      "[19 epo]: TRAIN:[loss=0.188]\n",
      "[20 epo]: TRAIN:[loss=0.118]\n",
      "[21 epo]: TRAIN:[loss=0.063]\n",
      "[22 epo]: TRAIN:[loss=0.031]\n",
      "[23 epo]: TRAIN:[loss=0.021]\n",
      "[24 epo]: TRAIN:[loss=0.027]\n",
      "[25 epo]: TRAIN:[loss=0.043]\n",
      "[26 epo]: TRAIN:[loss=0.062]\n",
      "[27 epo]: TRAIN:[loss=0.079]\n",
      "[28 epo]: TRAIN:[loss=0.090]\n",
      "[29 epo]: TRAIN:[loss=0.094]\n",
      "[30 epo]: TRAIN:[loss=0.089]\n",
      "[31 epo]: TRAIN:[loss=0.079]\n",
      "[32 epo]: TRAIN:[loss=0.064]\n",
      "[33 epo]: TRAIN:[loss=0.048]\n",
      "[34 epo]: TRAIN:[loss=0.033]\n",
      "[35 epo]: TRAIN:[loss=0.020]\n",
      "[36 epo]: TRAIN:[loss=0.012]\n",
      "[37 epo]: TRAIN:[loss=0.009]\n",
      "[38 epo]: TRAIN:[loss=0.009]\n",
      "[39 epo]: TRAIN:[loss=0.012]\n",
      "[40 epo]: TRAIN:[loss=0.016]\n",
      "[41 epo]: TRAIN:[loss=0.020]\n",
      "[42 epo]: TRAIN:[loss=0.022]\n",
      "[43 epo]: TRAIN:[loss=0.022]\n",
      "[44 epo]: TRAIN:[loss=0.019]\n",
      "[45 epo]: TRAIN:[loss=0.016]\n",
      "[46 epo]: TRAIN:[loss=0.012]\n",
      "[47 epo]: TRAIN:[loss=0.008]\n",
      "[48 epo]: TRAIN:[loss=0.005]\n",
      "[49 epo]: TRAIN:[loss=0.003]\n",
      "[50 epo]: TRAIN:[loss=0.003]\n",
      "[51 epo]: TRAIN:[loss=0.003]\n",
      "[52 epo]: TRAIN:[loss=0.004]\n",
      "[53 epo]: TRAIN:[loss=0.005]\n",
      "[54 epo]: TRAIN:[loss=0.006]\n",
      "[55 epo]: TRAIN:[loss=0.006]\n",
      "[56 epo]: TRAIN:[loss=0.006]\n",
      "[57 epo]: TRAIN:[loss=0.006]\n",
      "[58 epo]: TRAIN:[loss=0.005]\n",
      "[59 epo]: TRAIN:[loss=0.004]\n",
      "[60 epo]: TRAIN:[loss=0.003]\n",
      "[61 epo]: TRAIN:[loss=0.002]\n",
      "[62 epo]: TRAIN:[loss=0.001]\n",
      "[63 epo]: TRAIN:[loss=0.001]\n",
      "[64 epo]: TRAIN:[loss=0.001]\n",
      "[65 epo]: TRAIN:[loss=0.001]\n",
      "[66 epo]: TRAIN:[loss=0.001]\n",
      "[67 epo]: TRAIN:[loss=0.001]\n",
      "[68 epo]: TRAIN:[loss=0.001]\n",
      "[69 epo]: TRAIN:[loss=0.001]\n",
      "[70 epo]: TRAIN:[loss=0.001]\n",
      "[71 epo]: TRAIN:[loss=0.001]\n",
      "[72 epo]: TRAIN:[loss=0.001]\n",
      "[73 epo]: TRAIN:[loss=0.001]\n",
      "[74 epo]: TRAIN:[loss=0.001]\n",
      "[75 epo]: TRAIN:[loss=0.000]\n",
      "[76 epo]: TRAIN:[loss=0.000]\n",
      "[77 epo]: TRAIN:[loss=0.000]\n",
      "[78 epo]: TRAIN:[loss=0.000]\n",
      "[79 epo]: TRAIN:[loss=0.000]\n",
      "[80 epo]: TRAIN:[loss=0.000]\n",
      "[81 epo]: TRAIN:[loss=0.000]\n",
      "[82 epo]: TRAIN:[loss=0.000]\n",
      "[83 epo]: TRAIN:[loss=0.000]\n",
      "[84 epo]: TRAIN:[loss=0.000]\n",
      "[85 epo]: TRAIN:[loss=0.000]\n",
      "[86 epo]: TRAIN:[loss=0.000]\n",
      "[87 epo]: TRAIN:[loss=0.000]\n",
      "[88 epo]: TRAIN:[loss=0.000]\n",
      "[89 epo]: TRAIN:[loss=0.000]\n",
      "[90 epo]: TRAIN:[loss=0.000]\n",
      "[91 epo]: TRAIN:[loss=0.000]\n",
      "[92 epo]: TRAIN:[loss=0.000]\n",
      "[93 epo]: TRAIN:[loss=0.000]\n",
      "[94 epo]: TRAIN:[loss=0.000]\n",
      "[95 epo]: TRAIN:[loss=0.000]\n",
      "[96 epo]: TRAIN:[loss=0.000]\n",
      "[97 epo]: TRAIN:[loss=0.000]\n",
      "[98 epo]: TRAIN:[loss=0.000]\n",
      "[99 epo]: TRAIN:[loss=0.000]\n",
      "[100 epo]: TRAIN:[loss=0.000]\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "label_model.train_model(lf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = label_model.predict(lf_matrix) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Accuracy and ROCAUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = dev_dataset.get_val('bias_label').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8777439024390244"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(predictions, gt) #ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8765432098765432"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(gt == predictions)/len(gt) #Accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
