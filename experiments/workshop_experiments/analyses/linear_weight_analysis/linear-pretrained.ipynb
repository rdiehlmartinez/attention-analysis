{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear weight analysis from a linear model trained on the large weakly labeled dataset, where the attention has been extracted from the pretrained BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"../../../workshop_experiments/attention_pretrained/large_data/concat_all/model_weights/linear-windowed-attention.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('f1.weight',\n",
       "              tensor([[-0.1803,  0.0370,  0.1251,  ...,  0.0485, -0.0409,  0.2689],\n",
       "                      [-0.0442, -0.0580, -0.0526,  ..., -0.1368, -0.0231, -0.0193],\n",
       "                      [-0.0577, -0.0292, -0.0330,  ...,  0.0874,  0.1158, -0.0968],\n",
       "                      ...,\n",
       "                      [-0.0890,  0.0353,  0.0518,  ..., -0.0246, -0.0109,  0.2641],\n",
       "                      [ 0.0921,  0.1613,  0.2801,  ..., -0.0031,  0.1427, -0.0581],\n",
       "                      [ 0.3360,  0.1887,  0.0465,  ...,  0.1261,  0.1885, -0.1304]],\n",
       "                     device='cuda:0')),\n",
       "             ('f1.bias',\n",
       "              tensor([ 0.0163, -0.0994, -0.0268,  0.3977, -0.0270, -0.0399, -0.0545,  0.0394,\n",
       "                      -0.1553, -0.0222,  0.2626,  0.3423,  0.0026, -0.1142,  0.3070],\n",
       "                     device='cuda:0')),\n",
       "             ('f2.weight',\n",
       "              tensor([[ 0.6584, -0.1798,  1.3266, -0.6571,  0.0736,  1.4193,  1.4660,  3.5430,\n",
       "                        2.7260,  1.5827, -0.9541, -0.8536,  0.7253,  3.3728, -0.6579]],\n",
       "                     device='cuda:0')),\n",
       "             ('f2.bias', tensor([-0.2910], device='cuda:0'))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 180])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['f1.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4897,  1.0427,  0.5426,  1.1156,  0.7356,  1.1256, -1.6700, -1.9723,\n",
       "         6.2024, -0.5142, -1.3770,  1.1462, -1.0289, -0.6548, -1.0667],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analysis of the first layer (15 )\n",
    "torch.sum(model['f1.weight'][:,:15],dim=0) # Framing is 1 epistemological is 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5565,  0.5228,  0.4986,  0.5779,  0.3773,  0.5772, -0.2338,  0.1052,\n",
      "        -0.2187, -0.4096,  0.2852,  0.1007,  0.0446,  0.2137, -0.1303])\n"
     ]
    }
   ],
   "source": [
    "running_sum = torch.zeros(15, dtype=torch.float)\n",
    "for i in range(12):\n",
    "    start_idx = i * 15\n",
    "    end_idx = (i+1) * 15\n",
    "    running_sum = running_sum + torch.sum(model['f1.weight'][:,start_idx:end_idx],dim=0).cpu()\n",
    "print(running_sum/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5565373301506042\n",
      "0.5228064656257629\n",
      "0.4986400604248047\n",
      "0.5779280066490173\n",
      "0.37726321816444397\n",
      "0.5772495865821838\n",
      "-0.2338053435087204\n",
      "0.105185367166996\n",
      "-0.21868400275707245\n",
      "-0.40956056118011475\n",
      "0.28524449467658997\n",
      "0.10071172565221786\n",
      "0.044627949595451355\n",
      "0.21371996402740479\n",
      "-0.13027894496917725\n"
     ]
    }
   ],
   "source": [
    "for i in running_sum/12:\n",
    "    print(i.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
