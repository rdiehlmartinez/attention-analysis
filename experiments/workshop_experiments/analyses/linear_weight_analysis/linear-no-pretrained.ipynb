{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear weight analysis from a linear model trained on the large weakly labeled dataset, where the attention has been extracted from a BERT model not pretrained on bias detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"../../../workshop_experiments/attention_no-pretrain/large_data/concat_all/model_weights/linear-windowed-attention.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('f1.weight',\n",
       "              tensor([[ 0.2441,  0.3389,  0.1353,  ...,  0.0888, -0.0596, -0.1424],\n",
       "                      [ 0.0161,  0.0614, -0.0104,  ..., -0.0753,  0.0011,  0.1191],\n",
       "                      [ 0.0808,  0.0628,  0.0843,  ...,  0.0313,  0.0014, -0.0647],\n",
       "                      ...,\n",
       "                      [ 0.1449, -0.0630, -0.0546,  ...,  0.0456, -0.0499,  0.0582],\n",
       "                      [ 0.0818,  0.1267,  0.1249,  ..., -0.0564, -0.0419, -0.0077],\n",
       "                      [ 0.0826,  0.0639,  0.0646,  ...,  0.0353,  0.0146, -0.0041]],\n",
       "                     device='cuda:0')),\n",
       "             ('f1.bias',\n",
       "              tensor([ 0.1247,  0.0546,  0.0995,  0.0320,  0.0184,  0.1033,  0.1833,  0.1352,\n",
       "                       0.0473, -0.0389,  0.1775,  0.0536,  0.2242,  0.1189,  0.1500],\n",
       "                     device='cuda:0')),\n",
       "             ('f2.weight',\n",
       "              tensor([[-1.7709, -1.6677,  1.0377, -1.4134, -1.3327, -2.2204, -1.8000,  1.2801,\n",
       "                       -1.6625,  0.8090,  0.8080, -1.9331,  1.8853, -2.4100,  1.1816]],\n",
       "                     device='cuda:0')),\n",
       "             ('f2.bias', tensor([0.1075], device='cuda:0'))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 180])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['f1.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1982,  1.2352,  1.1688,  1.2211,  0.9052,  1.0749,  3.9061, -0.4757,\n",
       "        -8.9996,  0.2985,  1.1857, -0.8404,  0.2110, -0.3476, -0.6776],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analysis of the first layer (15 )\n",
    "torch.sum(model['f1.weight'][:,:15],dim=0) # Framing is 1 epistemological is 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2552,  0.3184,  0.3716,  0.2921,  0.3098, -0.1689, -0.4891, -1.8051,\n",
      "        -0.6030, -1.0823, -0.0882, -0.3554, -0.0412, -0.0746,  0.1774])\n"
     ]
    }
   ],
   "source": [
    "running_sum = torch.zeros(15, dtype=torch.float)\n",
    "for i in range(12):\n",
    "    start_idx = i * 15\n",
    "    end_idx = (i+1) * 15\n",
    "    running_sum = running_sum + torch.sum(model['f1.weight'][:,start_idx:end_idx],dim=0).cpu()\n",
    "print(running_sum/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25521594285964966\n",
      "0.31844648718833923\n",
      "0.3715747594833374\n",
      "0.29207345843315125\n",
      "0.309754341840744\n",
      "-0.16885323822498322\n",
      "-0.48910796642303467\n",
      "-1.8051198720932007\n",
      "-0.6029573082923889\n",
      "-1.0822745561599731\n",
      "-0.08822081238031387\n",
      "-0.3553757667541504\n",
      "-0.041196953505277634\n",
      "-0.07459313422441483\n",
      "0.17738713324069977\n"
     ]
    }
   ],
   "source": [
    "for i in running_sum/12:\n",
    "    print(i.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
