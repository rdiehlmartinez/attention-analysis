{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging last 4 attention distributions using a GRU \n",
    "\n",
    "We average together the last 4 attention distributions of the input. Our classification model is an LSTM that reads in the attention distribution for each word sequentially. In general the max sequence lenght is 80 which means that our model will read in 80 data points which are each 80 dimensional. \n",
    "\n",
    "\n",
    "#### Notes\n",
    "* One remaining question is how can we experiment with different number of attention heads. In general the extract attention scores function seems to have some bugs that need ironing out - such as only being able to pass in a batch size of 1 into the extraction schema.\n",
    "\n",
    "* We also would like to eventually use a transformer architecture on top of the attention distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../../../../..\")\n",
    "import torch \n",
    "from src.experiment import AttentionExperiment, ClassificationExperiment\n",
    "from src.dataset import ExperimentDataset\n",
    "from src.params import Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params.read_params(\"experiment_params.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/19/2020 20:00:06 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "386it [00:00, 4913.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading in the dataset that we are using in this experiments \n",
    "# typically this dataset is the small set of ground-truth labels\n",
    "dataset = ExperimentDataset.init_dataset(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Length: 324 Keys: dict_keys(['pre_ids', 'masks', 'pre_lens', 'post_in_ids', 'post_out_ids', 'pre_tok_label_ids', 'post_tok_label_ids', 'rel_ids', 'pos_ids', 'categories', 'index', 'bias_label'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_dataloader = dataset.return_dataloader() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Experiment: \n",
    "* Is a class that wraps useful methods to extract attention distributions from a given BERT-based model \n",
    "* In the config file the user needs to specify a .ckpt file for a trained BERT-based model from which \n",
    "     we want to extract attention scores\n",
    "* The user needs to instantiate the attention experiment with a function that tells the model how to run \n",
    " inference on the given model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/19/2020 20:00:07 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The len of our vocabulary is 30523\n",
      "Cuda is set to true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/19/2020 20:00:07 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at ./cache/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "02/19/2020 20:00:07 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file ./cache/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp0e0eyydy\n",
      "02/19/2020 20:00:11 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "02/19/2020 20:00:18 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForMultitaskWithFeaturesOnTop not initialized from pretrained model: ['tok_classifier.out.0.weight', 'tok_classifier.out.0.bias', 'tok_classifier.enricher.0.weight', 'tok_classifier.enricher.0.bias', 'cls_classifier.weight', 'cls_classifier.bias']\n",
      "02/19/2020 20:00:18 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMultitaskWithFeaturesOnTop: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "/u/nlp/anaconda/main/anaconda3/envs/bias_classification/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "02/19/2020 20:00:19 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at ./cache/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "02/19/2020 20:00:19 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file ./cache/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp5tsh8dbq\n",
      "02/19/2020 20:00:23 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully loaded in attention experiment!\n"
     ]
    }
   ],
   "source": [
    "attention_experiment = AttentionExperiment.initialize_attention_experiment(params.intermediary_task, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Length: 324 Keys: dict_keys(['pre_ids', 'masks', 'pre_lens', 'post_in_ids', 'post_out_ids', 'pre_tok_label_ids', 'post_tok_label_ids', 'rel_ids', 'pos_ids', 'categories', 'index', 'bias_label'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract_attention_scores() works out of the box because the attention experiment has the config file saved, and knows what BERT model to use/load in, which layers to extract the attention scores from, and what the inference function is that should be used on this particular BERT model.\n",
    "\n",
    "Attention_scores is then a list of dictionaries. The keys in this dictionary are the specific layers of a BERT model and the values are the corresponding attention distributions extracted from that particular layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43468d19774b44fbbedb9c8b5fd98c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "attention_scores = attention_experiment.extract_attention_scores(attention_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.attention_utils import avg_attention_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_attention = avg_attention_dist(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 80, 80])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_attention[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_avg_attention = torch.stack(avg_attention).squeeze()\n",
    "# squeezes from [324, 1, 1, 80, 80] --> [324, 80, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.add_data(stacked_avg_attention, \"attention_dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shuffle_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Length: 324 Keys: dict_keys(['pre_ids', 'masks', 'pre_lens', 'post_in_ids', 'post_out_ids', 'pre_tok_label_ids', 'post_tok_label_ids', 'rel_ids', 'pos_ids', 'categories', 'index', 'bias_label', 'attention_dist'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is where the classification experiment starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a classification experiment that contains useful methods for classifying bias based on the attention distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_experiment = ClassificationExperiment.init_cls_experiment(params.final_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.classification_utils import run_bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127cc7e0b9db4aa7a1e8f170ecbc7803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Cross Validation Iteration', max=3, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91666713358f49c9b88edf1025ed8c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epochs', max=150, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 ; Loss 0.6971462965011597 \n",
      "Step: 3 ; Loss 0.692383885383606 \n",
      "Step: 6 ; Loss 0.6503917574882507 \n",
      "Step: 0 ; Loss 0.6304763555526733 \n",
      "Step: 3 ; Loss 0.7080031633377075 \n",
      "Step: 6 ; Loss 0.6238500475883484 \n",
      "Step: 0 ; Loss 0.6031494736671448 \n",
      "Step: 3 ; Loss 0.7197659611701965 \n",
      "Step: 6 ; Loss 0.6223848462104797 \n",
      "Step: 0 ; Loss 0.603582501411438 \n",
      "Step: 3 ; Loss 0.7072267532348633 \n",
      "Step: 6 ; Loss 0.6254379153251648 \n",
      "Step: 0 ; Loss 0.6075143814086914 \n",
      "Step: 3 ; Loss 0.6986983418464661 \n",
      "Step: 6 ; Loss 0.6240379810333252 \n",
      "Step: 0 ; Loss 0.6037139892578125 \n",
      "Step: 3 ; Loss 0.6921854615211487 \n",
      "Step: 6 ; Loss 0.6181116104125977 \n",
      "Step: 0 ; Loss 0.594451904296875 \n",
      "Step: 3 ; Loss 0.6795854568481445 \n",
      "Step: 6 ; Loss 0.6126923561096191 \n",
      "Step: 0 ; Loss 0.5786746144294739 \n",
      "Step: 3 ; Loss 0.6381656527519226 \n",
      "Step: 6 ; Loss 0.5993362665176392 \n",
      "Step: 0 ; Loss 0.5655835270881653 \n",
      "Step: 3 ; Loss 0.5424614548683167 \n",
      "Step: 6 ; Loss 0.5923318862915039 \n",
      "Step: 0 ; Loss 0.5652759075164795 \n",
      "Step: 3 ; Loss 0.4722570776939392 \n",
      "Step: 6 ; Loss 0.585064172744751 \n",
      "Step: 0 ; Loss 0.5531732439994812 \n",
      "Step: 3 ; Loss 0.4559176564216614 \n",
      "Step: 6 ; Loss 0.5770370364189148 \n",
      "Step: 0 ; Loss 0.5414701700210571 \n",
      "Step: 3 ; Loss 0.44150203466415405 \n",
      "Step: 6 ; Loss 0.5806043148040771 \n",
      "Step: 0 ; Loss 0.526577889919281 \n",
      "Step: 3 ; Loss 0.4269413352012634 \n",
      "Step: 6 ; Loss 0.578788161277771 \n",
      "Step: 0 ; Loss 0.5098552703857422 \n",
      "Step: 3 ; Loss 0.41828614473342896 \n",
      "Step: 6 ; Loss 0.5757113695144653 \n",
      "Step: 0 ; Loss 0.4956301748752594 \n",
      "Step: 3 ; Loss 0.4076350927352905 \n",
      "Step: 6 ; Loss 0.5752348303794861 \n",
      "Step: 0 ; Loss 0.49630069732666016 \n",
      "Step: 3 ; Loss 0.39704370498657227 \n",
      "Step: 6 ; Loss 0.5700912475585938 \n",
      "Step: 0 ; Loss 0.48597386479377747 \n",
      "Step: 3 ; Loss 0.37219253182411194 \n",
      "Step: 6 ; Loss 0.5620994567871094 \n",
      "Step: 0 ; Loss 0.4776865541934967 \n",
      "Step: 3 ; Loss 0.3495703637599945 \n",
      "Step: 6 ; Loss 0.5523360371589661 \n",
      "Step: 0 ; Loss 0.466172993183136 \n",
      "Step: 3 ; Loss 0.3395961821079254 \n",
      "Step: 6 ; Loss 0.5472487807273865 \n",
      "Step: 0 ; Loss 0.44505196809768677 \n",
      "Step: 3 ; Loss 0.3340792953968048 \n",
      "Step: 6 ; Loss 0.5165185928344727 \n",
      "Step: 0 ; Loss 0.44002771377563477 \n",
      "Step: 3 ; Loss 0.32090455293655396 \n",
      "Step: 6 ; Loss 0.5282993912696838 \n",
      "Step: 0 ; Loss 0.5227890610694885 \n",
      "Step: 3 ; Loss 0.3341057002544403 \n",
      "Step: 6 ; Loss 0.5441347360610962 \n",
      "Step: 0 ; Loss 0.4599808156490326 \n",
      "Step: 3 ; Loss 0.3822386860847473 \n",
      "Step: 6 ; Loss 0.4675958752632141 \n",
      "Step: 0 ; Loss 0.4537303149700165 \n",
      "Step: 3 ; Loss 0.3377062976360321 \n",
      "Step: 6 ; Loss 0.45351582765579224 \n",
      "Step: 0 ; Loss 0.4168860614299774 \n",
      "Step: 3 ; Loss 0.311510294675827 \n",
      "Step: 6 ; Loss 0.5308132171630859 \n",
      "Step: 0 ; Loss 0.4396481513977051 \n",
      "Step: 3 ; Loss 0.3695294260978699 \n",
      "Step: 6 ; Loss 0.4381093382835388 \n",
      "Step: 0 ; Loss 0.4284555912017822 \n",
      "Step: 3 ; Loss 0.31207484006881714 \n",
      "Step: 6 ; Loss 0.4284500479698181 \n",
      "Step: 0 ; Loss 0.38374292850494385 \n",
      "Step: 3 ; Loss 0.3258095383644104 \n",
      "Step: 6 ; Loss 0.42297643423080444 \n",
      "Step: 0 ; Loss 0.41439080238342285 \n",
      "Step: 3 ; Loss 0.2381911426782608 \n",
      "Step: 6 ; Loss 0.3730143904685974 \n",
      "Step: 0 ; Loss 0.3751215636730194 \n",
      "Step: 3 ; Loss 0.25019872188568115 \n",
      "Step: 6 ; Loss 0.37202155590057373 \n",
      "Step: 0 ; Loss 0.3410142660140991 \n",
      "Step: 3 ; Loss 0.21941110491752625 \n",
      "Step: 6 ; Loss 0.3462930917739868 \n",
      "Step: 0 ; Loss 0.38148707151412964 \n",
      "Step: 3 ; Loss 0.3649529218673706 \n",
      "Step: 6 ; Loss 0.42791926860809326 \n",
      "Step: 0 ; Loss 0.40414711833000183 \n",
      "Step: 3 ; Loss 0.3891739547252655 \n",
      "Step: 6 ; Loss 0.3882361650466919 \n",
      "Step: 0 ; Loss 0.3320760130882263 \n",
      "Step: 3 ; Loss 0.21818552911281586 \n",
      "Step: 6 ; Loss 0.3871859610080719 \n",
      "Step: 0 ; Loss 0.3337975740432739 \n",
      "Step: 3 ; Loss 0.22973674535751343 \n",
      "Step: 6 ; Loss 0.4200814366340637 \n",
      "Step: 0 ; Loss 0.38290640711784363 \n",
      "Step: 3 ; Loss 0.1838914304971695 \n",
      "Step: 6 ; Loss 0.3771466910839081 \n",
      "Step: 0 ; Loss 0.3252415359020233 \n",
      "Step: 3 ; Loss 0.206000417470932 \n",
      "Step: 6 ; Loss 0.34675702452659607 \n",
      "Step: 0 ; Loss 0.2898136377334595 \n",
      "Step: 3 ; Loss 0.17256276309490204 \n",
      "Step: 6 ; Loss 0.39636602997779846 \n",
      "Step: 0 ; Loss 0.33381980657577515 \n",
      "Step: 3 ; Loss 0.32255488634109497 \n",
      "Step: 6 ; Loss 0.4028841555118561 \n",
      "Step: 0 ; Loss 0.31827059388160706 \n",
      "Step: 3 ; Loss 0.3688579201698303 \n",
      "Step: 6 ; Loss 0.45629987120628357 \n",
      "Step: 0 ; Loss 0.4758462607860565 \n",
      "Step: 3 ; Loss 0.30496546626091003 \n",
      "Step: 6 ; Loss 0.43111059069633484 \n",
      "Step: 0 ; Loss 0.3433651030063629 \n",
      "Step: 3 ; Loss 0.37908631563186646 \n",
      "Step: 6 ; Loss 0.38690048456192017 \n",
      "Step: 0 ; Loss 0.31953978538513184 \n",
      "Step: 3 ; Loss 0.2909708619117737 \n",
      "Step: 6 ; Loss 0.3578757345676422 \n",
      "Step: 0 ; Loss 0.2927093207836151 \n",
      "Step: 3 ; Loss 0.31302523612976074 \n",
      "Step: 6 ; Loss 0.3319067358970642 \n",
      "Step: 0 ; Loss 0.29104259610176086 \n",
      "Step: 3 ; Loss 0.18517211079597473 \n",
      "Step: 6 ; Loss 0.37212756276130676 \n",
      "Step: 0 ; Loss 0.23522363603115082 \n",
      "Step: 3 ; Loss 0.19239577651023865 \n",
      "Step: 6 ; Loss 0.2879857122898102 \n",
      "Step: 0 ; Loss 0.25335195660591125 \n",
      "Step: 3 ; Loss 0.1744496375322342 \n",
      "Step: 6 ; Loss 0.27901795506477356 \n",
      "Step: 0 ; Loss 0.24167659878730774 \n",
      "Step: 3 ; Loss 0.15854902565479279 \n",
      "Step: 6 ; Loss 0.2519036531448364 \n",
      "Step: 0 ; Loss 0.20516690611839294 \n",
      "Step: 3 ; Loss 0.14213597774505615 \n",
      "Step: 6 ; Loss 0.25499624013900757 \n",
      "Step: 0 ; Loss 0.19889238476753235 \n",
      "Step: 3 ; Loss 0.1661687046289444 \n",
      "Step: 6 ; Loss 0.2797463834285736 \n",
      "Step: 0 ; Loss 0.21216683089733124 \n",
      "Step: 3 ; Loss 0.2053605616092682 \n",
      "Step: 6 ; Loss 0.3307923674583435 \n",
      "Step: 0 ; Loss 0.2539654076099396 \n",
      "Step: 3 ; Loss 0.18049921095371246 \n",
      "Step: 6 ; Loss 0.27093806862831116 \n",
      "Step: 0 ; Loss 0.22465136647224426 \n",
      "Step: 3 ; Loss 0.1480511873960495 \n",
      "Step: 6 ; Loss 0.23628179728984833 \n",
      "Step: 0 ; Loss 0.2304908037185669 \n",
      "Step: 3 ; Loss 0.14239320158958435 \n",
      "Step: 6 ; Loss 0.2540091276168823 \n",
      "Step: 0 ; Loss 0.20626819133758545 \n",
      "Step: 3 ; Loss 0.16781504452228546 \n",
      "Step: 6 ; Loss 0.2155713587999344 \n",
      "Step: 0 ; Loss 0.18998806178569794 \n",
      "Step: 3 ; Loss 0.1242263913154602 \n",
      "Step: 6 ; Loss 0.23741866648197174 \n",
      "Step: 0 ; Loss 0.19710597395896912 \n",
      "Step: 3 ; Loss 0.14692378044128418 \n",
      "Step: 6 ; Loss 0.2663457989692688 \n",
      "Step: 0 ; Loss 0.30323487520217896 \n",
      "Step: 3 ; Loss 0.20281337201595306 \n",
      "Step: 6 ; Loss 0.2700989842414856 \n",
      "Step: 0 ; Loss 0.3081226348876953 \n",
      "Step: 3 ; Loss 0.15529881417751312 \n",
      "Step: 6 ; Loss 0.24380064010620117 \n",
      "Step: 0 ; Loss 0.23455016314983368 \n",
      "Step: 3 ; Loss 0.12385813891887665 \n",
      "Step: 6 ; Loss 0.3041013777256012 \n",
      "Step: 0 ; Loss 0.26345595717430115 \n",
      "Step: 3 ; Loss 0.10742276906967163 \n",
      "Step: 6 ; Loss 0.21810731291770935 \n",
      "Step: 0 ; Loss 0.27917739748954773 \n",
      "Step: 3 ; Loss 0.10338596999645233 \n",
      "Step: 6 ; Loss 0.2158529907464981 \n",
      "Step: 0 ; Loss 0.21178749203681946 \n",
      "Step: 3 ; Loss 0.11340474337339401 \n",
      "Step: 6 ; Loss 0.21772490441799164 \n",
      "Step: 0 ; Loss 0.20569829642772675 \n",
      "Step: 3 ; Loss 0.10075534880161285 \n",
      "Step: 6 ; Loss 0.18364794552326202 \n",
      "Step: 0 ; Loss 0.20938043296337128 \n",
      "Step: 3 ; Loss 0.09135138243436813 \n",
      "Step: 6 ; Loss 0.18276885151863098 \n",
      "Step: 0 ; Loss 0.19206176698207855 \n",
      "Step: 3 ; Loss 0.09440475702285767 \n",
      "Step: 6 ; Loss 0.17651833593845367 \n",
      "Step: 0 ; Loss 0.16384568810462952 \n",
      "Step: 3 ; Loss 0.09204772859811783 \n",
      "Step: 6 ; Loss 0.15731196105480194 \n",
      "Step: 0 ; Loss 0.1637980043888092 \n",
      "Step: 3 ; Loss 0.08723368495702744 \n",
      "Step: 6 ; Loss 0.14548920094966888 \n",
      "Step: 0 ; Loss 0.15832366049289703 \n",
      "Step: 3 ; Loss 0.0811987817287445 \n",
      "Step: 6 ; Loss 0.14386150240898132 \n",
      "Step: 0 ; Loss 0.15166492760181427 \n",
      "Step: 3 ; Loss 0.07692091912031174 \n",
      "Step: 6 ; Loss 0.13402335345745087 \n",
      "Step: 0 ; Loss 0.15325723588466644 \n",
      "Step: 3 ; Loss 0.07190417498350143 \n",
      "Step: 6 ; Loss 0.12655414640903473 \n",
      "Step: 0 ; Loss 0.15919426083564758 \n",
      "Step: 3 ; Loss 0.10205794870853424 \n",
      "Step: 6 ; Loss 0.1428811103105545 \n",
      "Step: 0 ; Loss 0.1525624543428421 \n",
      "Step: 3 ; Loss 0.08415503054857254 \n",
      "Step: 6 ; Loss 0.11545000970363617 \n",
      "Step: 0 ; Loss 0.16195258498191833 \n",
      "Step: 3 ; Loss 0.0751664936542511 \n",
      "Step: 6 ; Loss 0.16197648644447327 \n",
      "Step: 0 ; Loss 0.13705578446388245 \n",
      "Step: 3 ; Loss 0.07367909699678421 \n",
      "Step: 6 ; Loss 0.1154983714222908 \n",
      "Step: 0 ; Loss 0.12316283583641052 \n",
      "Step: 3 ; Loss 0.08646390587091446 \n",
      "Step: 6 ; Loss 0.1071561723947525 \n",
      "Step: 0 ; Loss 0.1660451740026474 \n",
      "Step: 3 ; Loss 0.06560230255126953 \n",
      "Step: 6 ; Loss 0.12887699902057648 \n",
      "Step: 0 ; Loss 0.10663550347089767 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3 ; Loss 0.28453874588012695 \n",
      "Step: 6 ; Loss 0.237224280834198 \n",
      "Step: 0 ; Loss 0.4630010426044464 \n",
      "Step: 3 ; Loss 0.2432796210050583 \n",
      "Step: 6 ; Loss 0.4021208584308624 \n",
      "Step: 0 ; Loss 0.43308526277542114 \n",
      "Step: 3 ; Loss 0.12418773770332336 \n",
      "Step: 6 ; Loss 0.27421438694000244 \n",
      "Step: 0 ; Loss 0.24169492721557617 \n",
      "Step: 3 ; Loss 0.17862758040428162 \n",
      "Step: 6 ; Loss 0.18237653374671936 \n",
      "Step: 0 ; Loss 0.3625430166721344 \n",
      "Step: 3 ; Loss 0.13350233435630798 \n",
      "Step: 6 ; Loss 0.2703002095222473 \n",
      "Step: 0 ; Loss 0.44789543747901917 \n",
      "Step: 3 ; Loss 0.38834428787231445 \n",
      "Step: 6 ; Loss 0.4613816440105438 \n",
      "Step: 0 ; Loss 0.18570470809936523 \n",
      "Step: 3 ; Loss 0.19440051913261414 \n",
      "Step: 6 ; Loss 0.12191767245531082 \n",
      "Step: 0 ; Loss 0.20709273219108582 \n",
      "Step: 3 ; Loss 0.17909613251686096 \n",
      "Step: 6 ; Loss 0.11623779684305191 \n",
      "Step: 0 ; Loss 0.13306193053722382 \n",
      "Step: 3 ; Loss 0.10298753529787064 \n",
      "Step: 6 ; Loss 0.09625814855098724 \n",
      "Step: 0 ; Loss 0.09912914037704468 \n",
      "Step: 3 ; Loss 0.07964907586574554 \n",
      "Step: 6 ; Loss 0.08059345930814743 \n",
      "Step: 0 ; Loss 0.10122201591730118 \n",
      "Step: 3 ; Loss 0.10107442736625671 \n",
      "Step: 6 ; Loss 0.07771936804056168 \n",
      "Step: 0 ; Loss 0.10181722790002823 \n",
      "Step: 3 ; Loss 0.07724521309137344 \n",
      "Step: 6 ; Loss 0.07807793468236923 \n",
      "Step: 0 ; Loss 0.09593084454536438 \n",
      "Step: 3 ; Loss 0.06740588694810867 \n",
      "Step: 6 ; Loss 0.06657541543245316 \n",
      "Step: 0 ; Loss 0.0770590752363205 \n",
      "Step: 3 ; Loss 0.06233565881848335 \n",
      "Step: 6 ; Loss 0.058298274874687195 \n",
      "Step: 0 ; Loss 0.07649766653776169 \n",
      "Step: 3 ; Loss 0.058597929775714874 \n",
      "Step: 6 ; Loss 0.058995041996240616 \n",
      "Step: 0 ; Loss 0.06914733350276947 \n",
      "Step: 3 ; Loss 0.04671650007367134 \n",
      "Step: 6 ; Loss 0.05285875126719475 \n",
      "Step: 0 ; Loss 0.06457658857107162 \n",
      "Step: 3 ; Loss 0.040839120745658875 \n",
      "Step: 6 ; Loss 0.051107008010149 \n",
      "Step: 0 ; Loss 0.06483159959316254 \n",
      "Step: 3 ; Loss 0.0455920435488224 \n",
      "Step: 6 ; Loss 0.04862053319811821 \n",
      "Step: 0 ; Loss 0.057340823113918304 \n",
      "Step: 3 ; Loss 0.04118639603257179 \n",
      "Step: 6 ; Loss 0.04487191140651703 \n",
      "Step: 0 ; Loss 0.05631124973297119 \n",
      "Step: 3 ; Loss 0.04446897283196449 \n",
      "Step: 6 ; Loss 0.04171884432435036 \n",
      "Step: 0 ; Loss 0.046174127608537674 \n",
      "Step: 3 ; Loss 0.03949372470378876 \n",
      "Step: 6 ; Loss 0.03921752795577049 \n",
      "Step: 0 ; Loss 0.03863905742764473 \n",
      "Step: 3 ; Loss 0.030917467549443245 \n",
      "Step: 6 ; Loss 0.03601452708244324 \n",
      "Step: 0 ; Loss 0.042776770889759064 \n",
      "Step: 3 ; Loss 0.03010193631052971 \n",
      "Step: 6 ; Loss 0.03325054049491882 \n",
      "Step: 0 ; Loss 0.038814205676317215 \n",
      "Step: 3 ; Loss 0.030367424711585045 \n",
      "Step: 6 ; Loss 0.031208882108330727 \n",
      "Step: 0 ; Loss 0.03285762295126915 \n",
      "Step: 3 ; Loss 0.03195076063275337 \n",
      "Step: 6 ; Loss 0.02955816313624382 \n",
      "Step: 0 ; Loss 0.03250802308320999 \n",
      "Step: 3 ; Loss 0.03118627890944481 \n",
      "Step: 6 ; Loss 0.027887588366866112 \n",
      "Step: 0 ; Loss 0.02630891092121601 \n",
      "Step: 3 ; Loss 0.027375707402825356 \n",
      "Step: 6 ; Loss 0.027029231190681458 \n",
      "Step: 0 ; Loss 0.025098657235503197 \n",
      "Step: 3 ; Loss 0.02383057028055191 \n",
      "Step: 6 ; Loss 0.025584807619452477 \n",
      "Step: 0 ; Loss 0.02292986772954464 \n",
      "Step: 3 ; Loss 0.024489669129252434 \n",
      "Step: 6 ; Loss 0.023110536858439445 \n",
      "Step: 0 ; Loss 0.022749638184905052 \n",
      "Step: 3 ; Loss 0.02532806247472763 \n",
      "Step: 6 ; Loss 0.022423386573791504 \n",
      "Step: 0 ; Loss 0.030050793662667274 \n",
      "Step: 3 ; Loss 0.020929353311657906 \n",
      "Step: 6 ; Loss 0.020930659025907516 \n",
      "Step: 0 ; Loss 0.02092721499502659 \n",
      "Step: 3 ; Loss 0.023064445704221725 \n",
      "Step: 6 ; Loss 0.01859109289944172 \n",
      "Step: 0 ; Loss 0.02165325917303562 \n",
      "Step: 3 ; Loss 0.019131310284137726 \n",
      "Step: 6 ; Loss 0.017959363758563995 \n",
      "Step: 0 ; Loss 0.018968677148222923 \n",
      "Step: 3 ; Loss 0.020290475338697433 \n",
      "Step: 6 ; Loss 0.0192307960242033 \n",
      "Step: 0 ; Loss 0.019344374537467957 \n",
      "Step: 3 ; Loss 0.017421165481209755 \n",
      "Step: 6 ; Loss 0.018180644139647484 \n",
      "Step: 0 ; Loss 0.024621792137622833 \n",
      "Step: 3 ; Loss 0.019090590998530388 \n",
      "Step: 6 ; Loss 0.047958485782146454 \n",
      "Step: 0 ; Loss 0.07516742497682571 \n",
      "Step: 3 ; Loss 0.028640856966376305 \n",
      "Step: 6 ; Loss 0.03289341181516647 \n",
      "Step: 0 ; Loss 0.04490497708320618 \n",
      "Step: 3 ; Loss 0.035770826041698456 \n",
      "Step: 6 ; Loss 0.03521681949496269 \n",
      "Step: 0 ; Loss 0.07437393814325333 \n",
      "Step: 3 ; Loss 0.030351191759109497 \n",
      "Step: 6 ; Loss 0.02864883281290531 \n",
      "Step: 0 ; Loss 0.0818447545170784 \n",
      "Step: 3 ; Loss 0.02063845284283161 \n",
      "Step: 6 ; Loss 0.10685564577579498 \n",
      "Step: 0 ; Loss 0.01657037064433098 \n",
      "Step: 3 ; Loss 0.017414942383766174 \n",
      "Step: 6 ; Loss 0.08809574693441391 \n",
      "Step: 0 ; Loss 0.04030550643801689 \n",
      "Step: 3 ; Loss 0.014802328310906887 \n",
      "Step: 6 ; Loss 0.10276102274656296 \n",
      "Step: 0 ; Loss 0.01817231811583042 \n",
      "Step: 3 ; Loss 0.4101608097553253 \n",
      "Step: 6 ; Loss 0.4335485100746155 \n",
      "Step: 0 ; Loss 0.6375239491462708 \n",
      "Step: 3 ; Loss 0.4061950445175171 \n",
      "Step: 6 ; Loss 0.646645188331604 \n",
      "Step: 0 ; Loss 0.5958116054534912 \n",
      "Step: 3 ; Loss 0.3906480669975281 \n",
      "Step: 6 ; Loss 0.7035112977027893 \n",
      "Step: 0 ; Loss 0.3839659094810486 \n",
      "Step: 3 ; Loss 0.3124150335788727 \n",
      "Step: 6 ; Loss 0.5619260668754578 \n",
      "Step: 0 ; Loss 0.32045748829841614 \n",
      "Step: 3 ; Loss 0.28815358877182007 \n",
      "Step: 6 ; Loss 0.3423866629600525 \n",
      "Step: 0 ; Loss 0.19656594097614288 \n",
      "Step: 3 ; Loss 0.22484470903873444 \n",
      "Step: 6 ; Loss 0.19269384443759918 \n",
      "Step: 0 ; Loss 0.2360316812992096 \n",
      "Step: 3 ; Loss 0.16735908389091492 \n",
      "Step: 6 ; Loss 0.15684741735458374 \n",
      "Step: 0 ; Loss 0.21401864290237427 \n",
      "Step: 3 ; Loss 0.12367789447307587 \n",
      "Step: 6 ; Loss 0.2019074708223343 \n",
      "Step: 0 ; Loss 0.12046369910240173 \n",
      "Step: 3 ; Loss 0.1389642357826233 \n",
      "Step: 6 ; Loss 0.10089952498674393 \n",
      "Step: 0 ; Loss 0.1993916928768158 \n",
      "Step: 3 ; Loss 0.08071394264698029 \n",
      "Step: 6 ; Loss 0.12476570159196854 \n",
      "Step: 0 ; Loss 0.12484566867351532 \n",
      "Step: 3 ; Loss 0.06759312003850937 \n",
      "Step: 6 ; Loss 0.08700430393218994 \n",
      "Step: 0 ; Loss 0.10042284429073334 \n",
      "Step: 3 ; Loss 0.05322424694895744 \n",
      "Step: 6 ; Loss 0.0658157542347908 \n",
      "Step: 0 ; Loss 0.07414714992046356 \n",
      "Step: 3 ; Loss 0.043045736849308014 \n",
      "Step: 6 ; Loss 0.045916151255369186 \n",
      "Step: 0 ; Loss 0.05805034935474396 \n",
      "Step: 3 ; Loss 0.044631119817495346 \n",
      "Step: 6 ; Loss 0.04347451403737068 \n",
      "Step: 0 ; Loss 0.04763253778219223 \n",
      "Step: 3 ; Loss 0.04554590955376625 \n",
      "Step: 6 ; Loss 0.03764320909976959 \n",
      "Step: 0 ; Loss 0.04182393103837967 \n",
      "Step: 3 ; Loss 0.022474633529782295 \n",
      "Step: 6 ; Loss 0.03419351577758789 \n",
      "Step: 0 ; Loss 0.037018127739429474 \n",
      "Step: 3 ; Loss 0.02260195091366768 \n",
      "Step: 6 ; Loss 0.033296216279268265 \n",
      "Step: 0 ; Loss 0.032444361597299576 \n",
      "Step: 3 ; Loss 0.021665532141923904 \n",
      "Step: 6 ; Loss 0.02975977212190628 \n",
      "Step: 0 ; Loss 0.025511736050248146 \n",
      "Step: 3 ; Loss 0.015328480862081051 \n",
      "Step: 6 ; Loss 0.02790122851729393 \n",
      "Step: 0 ; Loss 0.02071225829422474 \n",
      "Step: 3 ; Loss 0.01516641117632389 \n",
      "Step: 6 ; Loss 0.02642746828496456 \n",
      "Step: 0 ; Loss 0.016460532322525978 \n",
      "Step: 3 ; Loss 0.01192878931760788 \n",
      "Step: 6 ; Loss 0.024071533232927322 \n",
      "Step: 0 ; Loss 0.01667928509414196 \n",
      "Step: 3 ; Loss 0.011788629926741123 \n",
      "Step: 6 ; Loss 0.02371082454919815 \n",
      "Step: 0 ; Loss 0.013477945700287819 \n",
      "Step: 3 ; Loss 0.013334015384316444 \n",
      "Step: 6 ; Loss 0.021742694079875946 \n",
      "Step: 0 ; Loss 0.015171241946518421 \n",
      "Step: 3 ; Loss 0.010502809658646584 \n",
      "Step: 6 ; Loss 0.0206754170358181 \n",
      "Step: 0 ; Loss 0.012361885979771614 \n",
      "Step: 3 ; Loss 0.009962046518921852 \n",
      "Step: 6 ; Loss 0.02075745351612568 \n",
      "Step: 0 ; Loss 0.012812438420951366 \n",
      "Step: 3 ; Loss 0.009863192215561867 \n",
      "Step: 6 ; Loss 0.01934761181473732 \n",
      "Step: 0 ; Loss 0.012262524105608463 \n",
      "Step: 3 ; Loss 0.010507870465517044 \n",
      "Step: 6 ; Loss 0.01836080104112625 \n",
      "Step: 0 ; Loss 0.011876813136041164 \n",
      "Step: 3 ; Loss 0.008712254464626312 \n",
      "Step: 6 ; Loss 0.017860081046819687 \n",
      "Step: 0 ; Loss 0.011289220303297043 \n",
      "Step: 3 ; Loss 0.010478821583092213 \n",
      "Step: 6 ; Loss 0.017756376415491104 \n",
      "Step: 0 ; Loss 0.011062227189540863 \n",
      "Step: 3 ; Loss 0.008052350953221321 \n",
      "Step: 6 ; Loss 0.01678464189171791 \n",
      "Step: 0 ; Loss 0.01099063828587532 \n",
      "Step: 3 ; Loss 0.0076506310142576694 \n",
      "Step: 6 ; Loss 0.016320932656526566 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f978d48efd49b4a328f41208b70f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epochs', max=150, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 ; Loss 0.6203060746192932 \n",
      "Step: 3 ; Loss 0.6005366444587708 \n",
      "Step: 6 ; Loss 0.5153704881668091 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.45306241512298584 \n",
      "Step: 3 ; Loss 0.4700659513473511 \n",
      "Step: 6 ; Loss 0.4514164924621582 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.37552985548973083 \n",
      "Step: 3 ; Loss 0.40404027700424194 \n",
      "Step: 6 ; Loss 0.4061206579208374 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.4437530040740967 \n",
      "Step: 3 ; Loss 0.41677039861679077 \n",
      "Step: 6 ; Loss 0.4714903235435486 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.32585734128952026 \n",
      "Step: 3 ; Loss 0.45106247067451477 \n",
      "Step: 6 ; Loss 0.521379292011261 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.3385026752948761 \n",
      "Step: 3 ; Loss 0.42848315834999084 \n",
      "Step: 6 ; Loss 0.446835994720459 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.27988821268081665 \n",
      "Step: 3 ; Loss 0.3764690160751343 \n",
      "Step: 6 ; Loss 0.385665625333786 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.21955370903015137 \n",
      "Step: 3 ; Loss 0.345446914434433 \n",
      "Step: 6 ; Loss 0.33195239305496216 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.22937993705272675 \n",
      "Step: 3 ; Loss 0.2948278784751892 \n",
      "Step: 6 ; Loss 0.32157236337661743 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.19273383915424347 \n",
      "Step: 3 ; Loss 0.3093165457248688 \n",
      "Step: 6 ; Loss 0.28531405329704285 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.23244526982307434 \n",
      "Step: 3 ; Loss 0.3524649441242218 \n",
      "Step: 6 ; Loss 0.28576481342315674 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.14551298320293427 \n",
      "Step: 3 ; Loss 0.3149057626724243 \n",
      "Step: 6 ; Loss 0.3492092490196228 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.25080496072769165 \n",
      "Step: 3 ; Loss 0.29168701171875 \n",
      "Step: 6 ; Loss 0.3688744902610779 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.17372944951057434 \n",
      "Step: 3 ; Loss 0.3233649432659149 \n",
      "Step: 6 ; Loss 0.26910245418548584 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.10032212734222412 \n",
      "Step: 3 ; Loss 0.2743942439556122 \n",
      "Step: 6 ; Loss 0.17369142174720764 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.11693637073040009 \n",
      "Step: 3 ; Loss 0.2993471622467041 \n",
      "Step: 6 ; Loss 0.12531901895999908 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.10627011209726334 \n",
      "Step: 3 ; Loss 0.2544254660606384 \n",
      "Step: 6 ; Loss 0.19365030527114868 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.09359679371118546 \n",
      "Step: 3 ; Loss 0.12370894104242325 \n",
      "Step: 6 ; Loss 0.15110544860363007 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.22643594443798065 \n",
      "Step: 3 ; Loss 0.27380800247192383 \n",
      "Step: 6 ; Loss 0.25417569279670715 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.20437781512737274 \n",
      "Step: 3 ; Loss 0.31328532099723816 \n",
      "Step: 6 ; Loss 0.19996315240859985 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.14153148233890533 \n",
      "Step: 3 ; Loss 0.2801305055618286 \n",
      "Step: 6 ; Loss 0.1683853566646576 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.0840064063668251 \n",
      "Step: 3 ; Loss 0.24869054555892944 \n",
      "Step: 6 ; Loss 0.15852054953575134 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.13902202248573303 \n",
      "Step: 3 ; Loss 0.3447646200656891 \n",
      "Step: 6 ; Loss 0.2473072111606598 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.09092522412538528 \n",
      "Step: 3 ; Loss 0.12896184623241425 \n",
      "Step: 6 ; Loss 0.0954592302441597 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.11842528730630875 \n",
      "Step: 3 ; Loss 0.13855963945388794 \n",
      "Step: 6 ; Loss 0.10079089552164078 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.1029457375407219 \n",
      "Step: 3 ; Loss 0.13566285371780396 \n",
      "Step: 6 ; Loss 0.06767337769269943 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.06281556934118271 \n",
      "Step: 3 ; Loss 0.12690405547618866 \n",
      "Step: 6 ; Loss 0.0645199716091156 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.05713993310928345 \n",
      "Step: 3 ; Loss 0.08325248211622238 \n",
      "Step: 6 ; Loss 0.07325678318738937 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.06481204926967621 \n",
      "Step: 3 ; Loss 0.07599182426929474 \n",
      "Step: 6 ; Loss 0.05133364722132683 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.04986900836229324 \n",
      "Step: 3 ; Loss 0.06012687087059021 \n",
      "Step: 6 ; Loss 0.06429626047611237 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.04972325637936592 \n",
      "Step: 3 ; Loss 0.04423774033784866 \n",
      "Step: 6 ; Loss 0.04340415075421333 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.044663213193416595 \n",
      "Step: 3 ; Loss 0.04673895239830017 \n",
      "Step: 6 ; Loss 0.0436210073530674 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.04436375945806503 \n",
      "Step: 3 ; Loss 0.039749179035425186 \n",
      "Step: 6 ; Loss 0.032957904040813446 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.04370395839214325 \n",
      "Step: 3 ; Loss 0.04220721125602722 \n",
      "Step: 6 ; Loss 0.030307073146104813 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.03764982894062996 \n",
      "Step: 3 ; Loss 0.028489714488387108 \n",
      "Step: 6 ; Loss 0.029177311807870865 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.029177922755479813 \n",
      "Step: 3 ; Loss 0.0306074358522892 \n",
      "Step: 6 ; Loss 0.023687535896897316 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.028971191495656967 \n",
      "Step: 3 ; Loss 0.033917851746082306 \n",
      "Step: 6 ; Loss 0.02759142592549324 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.0307710450142622 \n",
      "Step: 3 ; Loss 0.025692977011203766 \n",
      "Step: 6 ; Loss 0.023076476529240608 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.02489328570663929 \n",
      "Step: 3 ; Loss 0.030905455350875854 \n",
      "Step: 6 ; Loss 0.01950647495687008 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.024372883141040802 \n",
      "Step: 3 ; Loss 0.02672875113785267 \n",
      "Step: 6 ; Loss 0.017965884879231453 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.02313217893242836 \n",
      "Step: 3 ; Loss 0.019989311695098877 \n",
      "Step: 6 ; Loss 0.0194375142455101 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.020271172747015953 \n",
      "Step: 3 ; Loss 0.02753356844186783 \n",
      "Step: 6 ; Loss 0.015967469662427902 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.021979227662086487 \n",
      "Step: 3 ; Loss 0.025592487305402756 \n",
      "Step: 6 ; Loss 0.014181762933731079 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.019194500520825386 \n",
      "Step: 3 ; Loss 0.020314548164606094 \n",
      "Step: 6 ; Loss 0.013648414984345436 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.018648244440555573 \n",
      "Step: 3 ; Loss 0.029563020914793015 \n",
      "Step: 6 ; Loss 0.012099199928343296 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.0179639533162117 \n",
      "Step: 3 ; Loss 0.027466922998428345 \n",
      "Step: 6 ; Loss 0.010851934552192688 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.018572034314274788 \n",
      "Step: 3 ; Loss 0.02029033936560154 \n",
      "Step: 6 ; Loss 0.01314781978726387 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.016681740060448647 \n",
      "Step: 3 ; Loss 0.016807762905955315 \n",
      "Step: 6 ; Loss 0.011114479973912239 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.01690981537103653 \n",
      "Step: 3 ; Loss 0.019516779109835625 \n",
      "Step: 6 ; Loss 0.009453135542571545 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.015799708664417267 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3 ; Loss 0.01815582625567913 \n",
      "Step: 6 ; Loss 0.009822087362408638 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.01623791828751564 \n",
      "Step: 3 ; Loss 0.016757501289248466 \n",
      "Step: 6 ; Loss 0.009022911079227924 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.014559631235897541 \n",
      "Step: 3 ; Loss 0.01793523132801056 \n",
      "Step: 6 ; Loss 0.009444532915949821 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.016218220815062523 \n",
      "Step: 3 ; Loss 0.016502689570188522 \n",
      "Step: 6 ; Loss 0.008997628465294838 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.01312455628067255 \n",
      "Step: 3 ; Loss 0.022040264680981636 \n",
      "Step: 6 ; Loss 0.007807966321706772 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.014352790080010891 \n",
      "Step: 3 ; Loss 0.0138898566365242 \n",
      "Step: 6 ; Loss 0.007782271597534418 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.011601760983467102 \n",
      "Step: 3 ; Loss 0.016737505793571472 \n",
      "Step: 6 ; Loss 0.007717500906437635 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.013241606764495373 \n",
      "Step: 3 ; Loss 0.01647152751684189 \n",
      "Step: 6 ; Loss 0.00816343817859888 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.012919659726321697 \n",
      "Step: 3 ; Loss 0.01512847375124693 \n",
      "Step: 6 ; Loss 0.007423108909279108 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.01164435874670744 \n",
      "Step: 3 ; Loss 0.01489588525146246 \n",
      "Step: 6 ; Loss 0.006754812318831682 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.010578446090221405 \n",
      "Step: 3 ; Loss 0.014472166076302528 \n",
      "Step: 6 ; Loss 0.006753838621079922 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.010285801254212856 \n",
      "Step: 3 ; Loss 0.013065777719020844 \n",
      "Step: 6 ; Loss 0.006827534642070532 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.010066419839859009 \n",
      "Step: 3 ; Loss 0.012798838317394257 \n",
      "Step: 6 ; Loss 0.007204340770840645 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.010240850038826466 \n",
      "Step: 3 ; Loss 0.01419561356306076 \n",
      "Step: 6 ; Loss 0.005985545925796032 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.00983843207359314 \n",
      "Step: 3 ; Loss 0.015604577027261257 \n",
      "Step: 6 ; Loss 0.005847279913723469 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.009522061794996262 \n",
      "Step: 3 ; Loss 0.011777934618294239 \n",
      "Step: 6 ; Loss 0.006114265415817499 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.010357475839555264 \n",
      "Step: 3 ; Loss 0.011028548702597618 \n",
      "Step: 6 ; Loss 0.005860847420990467 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.009017426520586014 \n",
      "Step: 3 ; Loss 0.012396564707159996 \n",
      "Step: 6 ; Loss 0.005790810100734234 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.009479186497628689 \n",
      "Step: 3 ; Loss 0.011763755232095718 \n",
      "Step: 6 ; Loss 0.005727194249629974 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.00846109539270401 \n",
      "Step: 3 ; Loss 0.012015110813081264 \n",
      "Step: 6 ; Loss 0.0053012920543551445 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.008854592218995094 \n",
      "Step: 3 ; Loss 0.010078310035169125 \n",
      "Step: 6 ; Loss 0.006484413053840399 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.008794418536126614 \n",
      "Step: 3 ; Loss 0.010914148762822151 \n",
      "Step: 6 ; Loss 0.005139632616192102 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.01380012184381485 \n",
      "Step: 3 ; Loss 0.010854735039174557 \n",
      "Step: 6 ; Loss 0.005173006094992161 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.008785504847764969 \n",
      "Step: 3 ; Loss 0.009832555428147316 \n",
      "Step: 6 ; Loss 0.005119547713547945 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.008200479671359062 \n",
      "Step: 3 ; Loss 0.009234137833118439 \n",
      "Step: 6 ; Loss 0.004896917846053839 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.008053655736148357 \n",
      "Step: 3 ; Loss 0.008235584944486618 \n",
      "Step: 6 ; Loss 0.00472258310765028 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.008096045814454556 \n",
      "Step: 3 ; Loss 0.008361654356122017 \n",
      "Step: 6 ; Loss 0.004936931189149618 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.12348349392414093 \n",
      "Step: 3 ; Loss 0.12253013253211975 \n",
      "Step: 6 ; Loss 0.005860426463186741 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.014000811614096165 \n",
      "Step: 3 ; Loss 0.05601780116558075 \n",
      "Step: 6 ; Loss 0.03552107885479927 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.008903543464839458 \n",
      "Step: 3 ; Loss 0.05063135549426079 \n",
      "Step: 6 ; Loss 0.023699481040239334 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.012063042260706425 \n",
      "Step: 3 ; Loss 0.023302612826228142 \n",
      "Step: 6 ; Loss 0.035503510385751724 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.09550981968641281 \n",
      "Step: 3 ; Loss 0.031147440895438194 \n",
      "Step: 6 ; Loss 0.2009749710559845 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.7133015394210815 \n",
      "Step: 3 ; Loss 1.229915976524353 \n",
      "Step: 6 ; Loss 2.281656265258789 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 1.796020269393921 \n",
      "Step: 3 ; Loss 0.7673417925834656 \n",
      "Step: 6 ; Loss 0.7494649887084961 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 1.3530893325805664 \n",
      "Step: 3 ; Loss 0.9404417276382446 \n",
      "Step: 6 ; Loss 0.6025772094726562 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.4665567874908447 \n",
      "Step: 3 ; Loss 0.7363137006759644 \n",
      "Step: 6 ; Loss 0.7490781545639038 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5716711282730103 \n",
      "Step: 3 ; Loss 0.6755495071411133 \n",
      "Step: 6 ; Loss 0.6063923835754395 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.6679712533950806 \n",
      "Step: 3 ; Loss 0.622827410697937 \n",
      "Step: 6 ; Loss 0.660900890827179 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.6799191236495972 \n",
      "Step: 3 ; Loss 0.6251954436302185 \n",
      "Step: 6 ; Loss 0.6490635871887207 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.6471431851387024 \n",
      "Step: 3 ; Loss 0.6384425759315491 \n",
      "Step: 6 ; Loss 0.6615542769432068 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.6196082830429077 \n",
      "Step: 3 ; Loss 0.6263728141784668 \n",
      "Step: 6 ; Loss 0.6662892699241638 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.6036595106124878 \n",
      "Step: 3 ; Loss 0.6294971704483032 \n",
      "Step: 6 ; Loss 0.6582713723182678 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5995074510574341 \n",
      "Step: 3 ; Loss 0.6486840844154358 \n",
      "Step: 6 ; Loss 0.6261467337608337 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5905479192733765 \n",
      "Step: 3 ; Loss 0.6283339858055115 \n",
      "Step: 6 ; Loss 0.6164721250534058 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5754714012145996 \n",
      "Step: 3 ; Loss 0.6139705181121826 \n",
      "Step: 6 ; Loss 0.6113645434379578 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5697337985038757 \n",
      "Step: 3 ; Loss 0.6017526388168335 \n",
      "Step: 6 ; Loss 0.6090301275253296 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5628021955490112 \n",
      "Step: 3 ; Loss 0.6092509031295776 \n",
      "Step: 6 ; Loss 0.5977572798728943 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.546867311000824 \n",
      "Step: 3 ; Loss 0.618454098701477 \n",
      "Step: 6 ; Loss 0.5874580144882202 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5327472686767578 \n",
      "Step: 3 ; Loss 0.6336866021156311 \n",
      "Step: 6 ; Loss 0.5973871350288391 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5363806486129761 \n",
      "Step: 3 ; Loss 0.6028444766998291 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6 ; Loss 0.5880367755889893 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5401203632354736 \n",
      "Step: 3 ; Loss 0.6106472015380859 \n",
      "Step: 6 ; Loss 0.5804495215415955 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5280779004096985 \n",
      "Step: 3 ; Loss 0.6108323931694031 \n",
      "Step: 6 ; Loss 0.5821245908737183 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5236149430274963 \n",
      "Step: 3 ; Loss 0.6262916326522827 \n",
      "Step: 6 ; Loss 0.5667567253112793 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.522339403629303 \n",
      "Step: 3 ; Loss 0.6176205277442932 \n",
      "Step: 6 ; Loss 0.5599054098129272 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5014303922653198 \n",
      "Step: 3 ; Loss 0.6319559216499329 \n",
      "Step: 6 ; Loss 0.5594376921653748 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5016419887542725 \n",
      "Step: 3 ; Loss 0.5942882895469666 \n",
      "Step: 6 ; Loss 0.5489991903305054 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.49528995156288147 \n",
      "Step: 3 ; Loss 0.6172929406166077 \n",
      "Step: 6 ; Loss 0.5385909676551819 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.4786132872104645 \n",
      "Step: 3 ; Loss 0.5896931886672974 \n",
      "Step: 6 ; Loss 0.5332294702529907 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.4855724275112152 \n",
      "Step: 3 ; Loss 0.5867229700088501 \n",
      "Step: 6 ; Loss 0.5323675870895386 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.45362555980682373 \n",
      "Step: 3 ; Loss 0.5597900748252869 \n",
      "Step: 6 ; Loss 0.5250863432884216 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.4549808204174042 \n",
      "Step: 3 ; Loss 0.5725473761558533 \n",
      "Step: 6 ; Loss 0.5239912271499634 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.45587706565856934 \n",
      "Step: 3 ; Loss 0.5705409646034241 \n",
      "Step: 6 ; Loss 0.5654035210609436 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.4560740888118744 \n",
      "Step: 3 ; Loss 0.6018325686454773 \n",
      "Step: 6 ; Loss 0.5261971950531006 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.44542837142944336 \n",
      "Step: 3 ; Loss 0.5713928937911987 \n",
      "Step: 6 ; Loss 0.544758677482605 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5667834281921387 \n",
      "Step: 3 ; Loss 0.5649597644805908 \n",
      "Step: 6 ; Loss 0.4772448241710663 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.6946994066238403 \n",
      "Step: 3 ; Loss 0.5934915542602539 \n",
      "Step: 6 ; Loss 0.4539237320423126 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.6883861422538757 \n",
      "Step: 3 ; Loss 0.5834853649139404 \n",
      "Step: 6 ; Loss 0.4278275668621063 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.6763523817062378 \n",
      "Step: 3 ; Loss 0.5653143525123596 \n",
      "Step: 6 ; Loss 0.4317671060562134 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.6473337411880493 \n",
      "Step: 3 ; Loss 0.5832309126853943 \n",
      "Step: 6 ; Loss 0.438502699136734 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.6271609663963318 \n",
      "Step: 3 ; Loss 0.5605791807174683 \n",
      "Step: 6 ; Loss 0.42128628492355347 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.6156102418899536 \n",
      "Step: 3 ; Loss 0.5412436127662659 \n",
      "Step: 6 ; Loss 0.40643537044525146 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.6132179498672485 \n",
      "Step: 3 ; Loss 0.5531191229820251 \n",
      "Step: 6 ; Loss 0.39638689160346985 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5970944762229919 \n",
      "Step: 3 ; Loss 0.5365147590637207 \n",
      "Step: 6 ; Loss 0.40250733494758606 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5898148417472839 \n",
      "Step: 3 ; Loss 0.5148624181747437 \n",
      "Step: 6 ; Loss 0.40643343329429626 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5853648781776428 \n",
      "Step: 3 ; Loss 0.5246933698654175 \n",
      "Step: 6 ; Loss 0.3992709517478943 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.584248960018158 \n",
      "Step: 3 ; Loss 0.5047669410705566 \n",
      "Step: 6 ; Loss 0.3855786919593811 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5876876711845398 \n",
      "Step: 3 ; Loss 0.4945574402809143 \n",
      "Step: 6 ; Loss 0.3921043276786804 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5584559440612793 \n",
      "Step: 3 ; Loss 0.4986953139305115 \n",
      "Step: 6 ; Loss 0.4068928360939026 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5551321506500244 \n",
      "Step: 3 ; Loss 0.4811806380748749 \n",
      "Step: 6 ; Loss 0.38153600692749023 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5454759001731873 \n",
      "Step: 3 ; Loss 0.4741578698158264 \n",
      "Step: 6 ; Loss 0.4201759099960327 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5321503281593323 \n",
      "Step: 3 ; Loss 0.44914576411247253 \n",
      "Step: 6 ; Loss 0.4035646319389343 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.511947751045227 \n",
      "Step: 3 ; Loss 0.4600713849067688 \n",
      "Step: 6 ; Loss 0.3933536410331726 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5024803280830383 \n",
      "Step: 3 ; Loss 0.4647597074508667 \n",
      "Step: 6 ; Loss 0.38648277521133423 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5294113755226135 \n",
      "Step: 3 ; Loss 0.4605451226234436 \n",
      "Step: 6 ; Loss 0.3870261609554291 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.502869188785553 \n",
      "Step: 3 ; Loss 0.4424061179161072 \n",
      "Step: 6 ; Loss 0.3798465132713318 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.5122049450874329 \n",
      "Step: 3 ; Loss 0.4258820712566376 \n",
      "Step: 6 ; Loss 0.3821696937084198 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.48420801758766174 \n",
      "Step: 3 ; Loss 0.4172581434249878 \n",
      "Step: 6 ; Loss 0.3781646192073822 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.4886224567890167 \n",
      "Step: 3 ; Loss 0.41062456369400024 \n",
      "Step: 6 ; Loss 0.40601956844329834 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.4346085786819458 \n",
      "Step: 3 ; Loss 0.40563708543777466 \n",
      "Step: 6 ; Loss 0.3905583918094635 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.44729816913604736 \n",
      "Step: 3 ; Loss 0.403839111328125 \n",
      "Step: 6 ; Loss 0.4111444354057312 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.4232726991176605 \n",
      "Step: 3 ; Loss 0.37614524364471436 \n",
      "Step: 6 ; Loss 0.39917728304862976 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.4294790029525757 \n",
      "Step: 3 ; Loss 0.39376258850097656 \n",
      "Step: 6 ; Loss 0.39107778668403625 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.4276694059371948 \n",
      "Step: 3 ; Loss 0.3699893057346344 \n",
      "Step: 6 ; Loss 0.3747541904449463 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.38710376620292664 \n",
      "Step: 3 ; Loss 0.37166574597358704 \n",
      "Step: 6 ; Loss 0.3476143181324005 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.46516039967536926 \n",
      "Step: 3 ; Loss 0.3508933484554291 \n",
      "Step: 6 ; Loss 0.3863467574119568 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.3826467990875244 \n",
      "Step: 3 ; Loss 0.3361046612262726 \n",
      "Step: 6 ; Loss 0.3581072986125946 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.3995912969112396 \n",
      "Step: 3 ; Loss 0.33623695373535156 \n",
      "Step: 6 ; Loss 0.356608122587204 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.38754433393478394 \n",
      "Step: 3 ; Loss 0.31712156534194946 \n",
      "Step: 6 ; Loss 0.3447791337966919 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.3692120313644409 \n",
      "Step: 3 ; Loss 0.32383906841278076 \n",
      "Step: 6 ; Loss 0.33163633942604065 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.3710719645023346 \n",
      "Step: 3 ; Loss 0.29306262731552124 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6 ; Loss 0.33660799264907837 \n",
      "All labels are of the same type – skipping AUC calculation\n",
      "Step: 0 ; Loss 0.36221766471862793 \n",
      "Step: 3 ; Loss 0.307378888130188 \n",
      "Step: 6 ; Loss 0.3631622791290283 \n",
      "All labels are of the same type – skipping AUC calculation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db039e6f02fd440cb40c7ae4624622d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epochs', max=150, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 ; Loss 0.7427797317504883 \n",
      "Step: 3 ; Loss 0.7244586944580078 \n",
      "Step: 6 ; Loss 0.6962272524833679 \n",
      "Step: 0 ; Loss 0.6670082807540894 \n",
      "Step: 3 ; Loss 0.6188298463821411 \n",
      "Step: 6 ; Loss 0.6358893513679504 \n",
      "Step: 0 ; Loss 0.6057889461517334 \n",
      "Step: 3 ; Loss 0.5435404777526855 \n",
      "Step: 6 ; Loss 0.5908763408660889 \n",
      "Step: 0 ; Loss 0.5493637323379517 \n",
      "Step: 3 ; Loss 0.5105730295181274 \n",
      "Step: 6 ; Loss 0.5732740759849548 \n",
      "Step: 0 ; Loss 0.5334495902061462 \n",
      "Step: 3 ; Loss 0.49505436420440674 \n",
      "Step: 6 ; Loss 0.5762150287628174 \n",
      "Step: 0 ; Loss 0.5289062261581421 \n",
      "Step: 3 ; Loss 0.44685128331184387 \n",
      "Step: 6 ; Loss 0.585297703742981 \n",
      "Step: 0 ; Loss 0.5207058787345886 \n",
      "Step: 3 ; Loss 0.44476521015167236 \n",
      "Step: 6 ; Loss 0.5501129627227783 \n",
      "Step: 0 ; Loss 0.5050820112228394 \n",
      "Step: 3 ; Loss 0.4345044791698456 \n",
      "Step: 6 ; Loss 0.5665379166603088 \n",
      "Step: 0 ; Loss 0.5130108594894409 \n",
      "Step: 3 ; Loss 0.4277929365634918 \n",
      "Step: 6 ; Loss 0.5424851775169373 \n",
      "Step: 0 ; Loss 0.5033265948295593 \n",
      "Step: 3 ; Loss 0.42826175689697266 \n",
      "Step: 6 ; Loss 0.5333763957023621 \n",
      "Step: 0 ; Loss 0.5039829611778259 \n",
      "Step: 3 ; Loss 0.4081089496612549 \n",
      "Step: 6 ; Loss 0.5297253727912903 \n",
      "Step: 0 ; Loss 0.4972573518753052 \n",
      "Step: 3 ; Loss 0.41360414028167725 \n",
      "Step: 6 ; Loss 0.5251793265342712 \n",
      "Step: 0 ; Loss 0.495019793510437 \n",
      "Step: 3 ; Loss 0.4050779640674591 \n",
      "Step: 6 ; Loss 0.5164302587509155 \n",
      "Step: 0 ; Loss 0.4971931278705597 \n",
      "Step: 3 ; Loss 0.4096016585826874 \n",
      "Step: 6 ; Loss 0.5170271992683411 \n",
      "Step: 0 ; Loss 0.4950528144836426 \n",
      "Step: 3 ; Loss 0.4038603901863098 \n",
      "Step: 6 ; Loss 0.5186133980751038 \n",
      "Step: 0 ; Loss 0.4904906749725342 \n",
      "Step: 3 ; Loss 0.3982052206993103 \n",
      "Step: 6 ; Loss 0.5115978121757507 \n",
      "Step: 0 ; Loss 0.49123409390449524 \n",
      "Step: 3 ; Loss 0.4008116126060486 \n",
      "Step: 6 ; Loss 0.5119380950927734 \n",
      "Step: 0 ; Loss 0.4857521057128906 \n",
      "Step: 3 ; Loss 0.38831087946891785 \n",
      "Step: 6 ; Loss 0.5064733624458313 \n",
      "Step: 0 ; Loss 0.47880688309669495 \n",
      "Step: 3 ; Loss 0.38745781779289246 \n",
      "Step: 6 ; Loss 0.5087631940841675 \n",
      "Step: 0 ; Loss 0.4682166874408722 \n",
      "Step: 3 ; Loss 0.3789392113685608 \n",
      "Step: 6 ; Loss 0.534639298915863 \n",
      "Step: 0 ; Loss 0.4669126570224762 \n",
      "Step: 3 ; Loss 0.37767305970191956 \n",
      "Step: 6 ; Loss 0.49741461873054504 \n",
      "Step: 0 ; Loss 0.4589204490184784 \n",
      "Step: 3 ; Loss 0.36216506361961365 \n",
      "Step: 6 ; Loss 0.47859519720077515 \n",
      "Step: 0 ; Loss 0.4475369155406952 \n",
      "Step: 3 ; Loss 0.3574712574481964 \n",
      "Step: 6 ; Loss 0.48731520771980286 \n",
      "Step: 0 ; Loss 0.44611579179763794 \n",
      "Step: 3 ; Loss 0.36077627539634705 \n",
      "Step: 6 ; Loss 0.488843709230423 \n",
      "Step: 0 ; Loss 0.45190194249153137 \n",
      "Step: 3 ; Loss 0.36218371987342834 \n",
      "Step: 6 ; Loss 0.49102577567100525 \n",
      "Step: 0 ; Loss 0.4382028877735138 \n",
      "Step: 3 ; Loss 0.3302546739578247 \n",
      "Step: 6 ; Loss 0.45879441499710083 \n",
      "Step: 0 ; Loss 0.4158810079097748 \n",
      "Step: 3 ; Loss 0.3492216467857361 \n",
      "Step: 6 ; Loss 0.4862716495990753 \n",
      "Step: 0 ; Loss 0.459757000207901 \n",
      "Step: 3 ; Loss 0.3503911793231964 \n",
      "Step: 6 ; Loss 0.4565393328666687 \n",
      "Step: 0 ; Loss 0.41991204023361206 \n",
      "Step: 3 ; Loss 0.3688327968120575 \n",
      "Step: 6 ; Loss 0.5637708902359009 \n",
      "Step: 0 ; Loss 0.7625797390937805 \n",
      "Step: 3 ; Loss 0.3061150014400482 \n",
      "Step: 6 ; Loss 0.5241250991821289 \n",
      "Step: 0 ; Loss 0.5568386912345886 \n",
      "Step: 3 ; Loss 0.3272591829299927 \n",
      "Step: 6 ; Loss 0.5023617148399353 \n",
      "Step: 0 ; Loss 0.5511707663536072 \n",
      "Step: 3 ; Loss 0.2853183150291443 \n",
      "Step: 6 ; Loss 0.4846205711364746 \n",
      "Step: 0 ; Loss 0.4357137680053711 \n",
      "Step: 3 ; Loss 0.30583974719047546 \n",
      "Step: 6 ; Loss 0.4968937635421753 \n",
      "Step: 0 ; Loss 0.4698276221752167 \n",
      "Step: 3 ; Loss 0.33977341651916504 \n",
      "Step: 6 ; Loss 0.48044759035110474 \n",
      "Step: 0 ; Loss 0.5203760266304016 \n",
      "Step: 3 ; Loss 0.47270166873931885 \n",
      "Step: 6 ; Loss 0.5497173070907593 \n",
      "Step: 0 ; Loss 0.5045033693313599 \n",
      "Step: 3 ; Loss 0.3524799048900604 \n",
      "Step: 6 ; Loss 0.5938977599143982 \n",
      "Step: 0 ; Loss 0.41597670316696167 \n",
      "Step: 3 ; Loss 0.4021502733230591 \n",
      "Step: 6 ; Loss 0.5157338380813599 \n",
      "Step: 0 ; Loss 0.47188645601272583 \n",
      "Step: 3 ; Loss 0.43831807374954224 \n",
      "Step: 6 ; Loss 0.5851261019706726 \n",
      "Step: 0 ; Loss 0.5396345853805542 \n",
      "Step: 3 ; Loss 0.5418657064437866 \n",
      "Step: 6 ; Loss 0.6112672090530396 \n",
      "Step: 0 ; Loss 0.48986512422561646 \n",
      "Step: 3 ; Loss 0.4894879162311554 \n",
      "Step: 6 ; Loss 0.5734147429466248 \n",
      "Step: 0 ; Loss 0.4341422915458679 \n",
      "Step: 3 ; Loss 0.46872207522392273 \n",
      "Step: 6 ; Loss 0.5662282109260559 \n",
      "Step: 0 ; Loss 0.437907338142395 \n",
      "Step: 3 ; Loss 0.43664854764938354 \n",
      "Step: 6 ; Loss 0.5045831799507141 \n",
      "Step: 0 ; Loss 0.4713020920753479 \n",
      "Step: 3 ; Loss 0.377905011177063 \n",
      "Step: 6 ; Loss 0.5115408301353455 \n",
      "Step: 0 ; Loss 0.5286347270011902 \n",
      "Step: 3 ; Loss 0.3403896391391754 \n",
      "Step: 6 ; Loss 0.44564294815063477 \n",
      "Step: 0 ; Loss 0.4207521975040436 \n",
      "Step: 3 ; Loss 0.3376580774784088 \n",
      "Step: 6 ; Loss 0.4116075336933136 \n",
      "Step: 0 ; Loss 0.40262022614479065 \n",
      "Step: 3 ; Loss 0.31108909845352173 \n",
      "Step: 6 ; Loss 0.43226417899131775 \n",
      "Step: 0 ; Loss 0.39304450154304504 \n",
      "Step: 3 ; Loss 0.31734439730644226 \n",
      "Step: 6 ; Loss 0.46689316630363464 \n",
      "Step: 0 ; Loss 0.3970499336719513 \n",
      "Step: 3 ; Loss 0.30067652463912964 \n",
      "Step: 6 ; Loss 0.4393378496170044 \n",
      "Step: 0 ; Loss 0.37156248092651367 \n",
      "Step: 3 ; Loss 0.2529572546482086 \n",
      "Step: 6 ; Loss 0.41899004578590393 \n",
      "Step: 0 ; Loss 0.3686627745628357 \n",
      "Step: 3 ; Loss 0.2260614037513733 \n",
      "Step: 6 ; Loss 0.41089996695518494 \n",
      "Step: 0 ; Loss 0.3749942481517792 \n",
      "Step: 3 ; Loss 0.2223743498325348 \n",
      "Step: 6 ; Loss 0.4081556499004364 \n",
      "Step: 0 ; Loss 0.35771575570106506 \n",
      "Step: 3 ; Loss 0.20884394645690918 \n",
      "Step: 6 ; Loss 0.39907246828079224 \n",
      "Step: 0 ; Loss 0.33689796924591064 \n",
      "Step: 3 ; Loss 0.2174995243549347 \n",
      "Step: 6 ; Loss 0.40011918544769287 \n",
      "Step: 0 ; Loss 0.3355810046195984 \n",
      "Step: 3 ; Loss 0.2079048603773117 \n",
      "Step: 6 ; Loss 0.38896527886390686 \n",
      "Step: 0 ; Loss 0.3221725523471832 \n",
      "Step: 3 ; Loss 0.19579635560512543 \n",
      "Step: 6 ; Loss 0.3855358958244324 \n",
      "Step: 0 ; Loss 0.30922237038612366 \n",
      "Step: 3 ; Loss 0.21253608167171478 \n",
      "Step: 6 ; Loss 0.4050011932849884 \n",
      "Step: 0 ; Loss 0.3641062080860138 \n",
      "Step: 3 ; Loss 0.23089607059955597 \n",
      "Step: 6 ; Loss 0.4288225471973419 \n",
      "Step: 0 ; Loss 0.47244489192962646 \n",
      "Step: 3 ; Loss 0.22668226063251495 \n",
      "Step: 6 ; Loss 0.33916398882865906 \n",
      "Step: 0 ; Loss 0.3625270426273346 \n",
      "Step: 3 ; Loss 0.17726518213748932 \n",
      "Step: 6 ; Loss 0.3542300760746002 \n",
      "Step: 0 ; Loss 0.40069642663002014 \n",
      "Step: 3 ; Loss 0.24531430006027222 \n",
      "Step: 6 ; Loss 0.32029977440834045 \n",
      "Step: 0 ; Loss 0.2846433222293854 \n",
      "Step: 3 ; Loss 0.23615851998329163 \n",
      "Step: 6 ; Loss 0.3728688657283783 \n",
      "Step: 0 ; Loss 0.2736259698867798 \n",
      "Step: 3 ; Loss 0.19993722438812256 \n",
      "Step: 6 ; Loss 0.3780183494091034 \n",
      "Step: 0 ; Loss 0.2551215589046478 \n",
      "Step: 3 ; Loss 0.1906619668006897 \n",
      "Step: 6 ; Loss 0.41691410541534424 \n",
      "Step: 0 ; Loss 0.38528475165367126 \n",
      "Step: 3 ; Loss 0.24783648550510406 \n",
      "Step: 6 ; Loss 0.46156224608421326 \n",
      "Step: 0 ; Loss 0.27534040808677673 \n",
      "Step: 3 ; Loss 0.2854805886745453 \n",
      "Step: 6 ; Loss 0.4174078404903412 \n",
      "Step: 0 ; Loss 0.2547309100627899 \n",
      "Step: 3 ; Loss 0.2402895838022232 \n",
      "Step: 6 ; Loss 0.6020724773406982 \n",
      "Step: 0 ; Loss 0.28223878145217896 \n",
      "Step: 3 ; Loss 0.22867648303508759 \n",
      "Step: 6 ; Loss 0.4260796904563904 \n",
      "Step: 0 ; Loss 0.2866746485233307 \n",
      "Step: 3 ; Loss 0.2341955304145813 \n",
      "Step: 6 ; Loss 0.3954974412918091 \n",
      "Step: 0 ; Loss 0.22505256533622742 \n",
      "Step: 3 ; Loss 0.26739218831062317 \n",
      "Step: 6 ; Loss 0.3634347915649414 \n",
      "Step: 0 ; Loss 0.24366948008537292 \n",
      "Step: 3 ; Loss 0.21558056771755219 \n",
      "Step: 6 ; Loss 0.3394308090209961 \n",
      "Step: 0 ; Loss 0.26035362482070923 \n",
      "Step: 3 ; Loss 0.20149314403533936 \n",
      "Step: 6 ; Loss 0.33571553230285645 \n",
      "Step: 0 ; Loss 0.21751628816127777 \n",
      "Step: 3 ; Loss 0.2043921947479248 \n",
      "Step: 6 ; Loss 0.3221275806427002 \n",
      "Step: 0 ; Loss 0.24077558517456055 \n",
      "Step: 3 ; Loss 0.19878849387168884 \n",
      "Step: 6 ; Loss 0.33556461334228516 \n",
      "Step: 0 ; Loss 0.24197766184806824 \n",
      "Step: 3 ; Loss 0.21253974735736847 \n",
      "Step: 6 ; Loss 0.2862548828125 \n",
      "Step: 0 ; Loss 0.24023857712745667 \n",
      "Step: 3 ; Loss 0.17062433063983917 \n",
      "Step: 6 ; Loss 0.31869208812713623 \n",
      "Step: 0 ; Loss 0.262399286031723 \n",
      "Step: 3 ; Loss 0.17028330266475677 \n",
      "Step: 6 ; Loss 0.3523705005645752 \n",
      "Step: 0 ; Loss 0.271688848733902 \n",
      "Step: 3 ; Loss 0.1487697809934616 \n",
      "Step: 6 ; Loss 0.3166208267211914 \n",
      "Step: 0 ; Loss 0.1990278959274292 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3 ; Loss 0.16159795224666595 \n",
      "Step: 6 ; Loss 0.2945966422557831 \n",
      "Step: 0 ; Loss 0.2049943208694458 \n",
      "Step: 3 ; Loss 0.1478855311870575 \n",
      "Step: 6 ; Loss 0.42051148414611816 \n",
      "Step: 0 ; Loss 0.3299188017845154 \n",
      "Step: 3 ; Loss 0.1381010264158249 \n",
      "Step: 6 ; Loss 0.28436699509620667 \n",
      "Step: 0 ; Loss 0.24290552735328674 \n",
      "Step: 3 ; Loss 0.13993990421295166 \n",
      "Step: 6 ; Loss 0.290988564491272 \n",
      "Step: 0 ; Loss 0.23732252418994904 \n",
      "Step: 3 ; Loss 0.12863464653491974 \n",
      "Step: 6 ; Loss 0.2624308466911316 \n",
      "Step: 0 ; Loss 0.2251303344964981 \n",
      "Step: 3 ; Loss 0.13168422877788544 \n",
      "Step: 6 ; Loss 0.21910101175308228 \n",
      "Step: 0 ; Loss 0.23520506918430328 \n",
      "Step: 3 ; Loss 0.12152355164289474 \n",
      "Step: 6 ; Loss 0.26561689376831055 \n",
      "Step: 0 ; Loss 0.22166453301906586 \n",
      "Step: 3 ; Loss 0.12412118166685104 \n",
      "Step: 6 ; Loss 0.25902894139289856 \n",
      "Step: 0 ; Loss 0.23977796733379364 \n",
      "Step: 3 ; Loss 0.13097158074378967 \n",
      "Step: 6 ; Loss 0.2600433826446533 \n",
      "Step: 0 ; Loss 0.20946532487869263 \n",
      "Step: 3 ; Loss 0.11169901490211487 \n",
      "Step: 6 ; Loss 0.25771579146385193 \n",
      "Step: 0 ; Loss 0.24993059039115906 \n",
      "Step: 3 ; Loss 0.11825629323720932 \n",
      "Step: 6 ; Loss 0.21414700150489807 \n",
      "Step: 0 ; Loss 0.1958799511194229 \n",
      "Step: 3 ; Loss 0.11571906507015228 \n",
      "Step: 6 ; Loss 0.20038063824176788 \n",
      "Step: 0 ; Loss 0.22651635110378265 \n",
      "Step: 3 ; Loss 0.11456028372049332 \n",
      "Step: 6 ; Loss 0.22057302296161652 \n",
      "Step: 0 ; Loss 0.21314294636249542 \n",
      "Step: 3 ; Loss 0.10510803014039993 \n",
      "Step: 6 ; Loss 0.1976391077041626 \n",
      "Step: 0 ; Loss 0.20373965799808502 \n",
      "Step: 3 ; Loss 0.11838492751121521 \n",
      "Step: 6 ; Loss 0.22617602348327637 \n",
      "Step: 0 ; Loss 0.18749503791332245 \n",
      "Step: 3 ; Loss 0.10946632921695709 \n",
      "Step: 6 ; Loss 0.2300129234790802 \n",
      "Step: 0 ; Loss 0.2326657921075821 \n",
      "Step: 3 ; Loss 0.1191289871931076 \n",
      "Step: 6 ; Loss 0.18825285136699677 \n",
      "Step: 0 ; Loss 0.1874423325061798 \n",
      "Step: 3 ; Loss 0.10108766704797745 \n",
      "Step: 6 ; Loss 0.2256840616464615 \n",
      "Step: 0 ; Loss 0.17635823786258698 \n",
      "Step: 3 ; Loss 0.09991119056940079 \n",
      "Step: 6 ; Loss 0.2245863378047943 \n",
      "Step: 0 ; Loss 0.1808067113161087 \n",
      "Step: 3 ; Loss 0.09346657991409302 \n",
      "Step: 6 ; Loss 0.21453574299812317 \n",
      "Step: 0 ; Loss 0.181585893034935 \n",
      "Step: 3 ; Loss 0.09225878864526749 \n",
      "Step: 6 ; Loss 0.213894784450531 \n",
      "Step: 0 ; Loss 0.16904324293136597 \n",
      "Step: 3 ; Loss 0.09103430807590485 \n",
      "Step: 6 ; Loss 0.21080511808395386 \n",
      "Step: 0 ; Loss 0.18698318302631378 \n",
      "Step: 3 ; Loss 0.12802758812904358 \n",
      "Step: 6 ; Loss 0.2465447038412094 \n",
      "Step: 0 ; Loss 0.27933719754219055 \n",
      "Step: 3 ; Loss 0.15573729574680328 \n",
      "Step: 6 ; Loss 0.23012229800224304 \n",
      "Step: 0 ; Loss 0.2099144458770752 \n",
      "Step: 3 ; Loss 0.1812659204006195 \n",
      "Step: 6 ; Loss 0.2370934784412384 \n",
      "Step: 0 ; Loss 0.18912500143051147 \n",
      "Step: 3 ; Loss 0.10809330642223358 \n",
      "Step: 6 ; Loss 0.21667872369289398 \n",
      "Step: 0 ; Loss 0.21001145243644714 \n",
      "Step: 3 ; Loss 0.10046287626028061 \n",
      "Step: 6 ; Loss 0.2046688199043274 \n",
      "Step: 0 ; Loss 0.18138322234153748 \n",
      "Step: 3 ; Loss 0.09638499468564987 \n",
      "Step: 6 ; Loss 0.2022792100906372 \n",
      "Step: 0 ; Loss 0.18386875092983246 \n",
      "Step: 3 ; Loss 0.18044789135456085 \n",
      "Step: 6 ; Loss 0.29763033986091614 \n",
      "Step: 0 ; Loss 0.21828968822956085 \n",
      "Step: 3 ; Loss 0.12148960679769516 \n",
      "Step: 6 ; Loss 0.21657726168632507 \n",
      "Step: 0 ; Loss 0.159159854054451 \n",
      "Step: 3 ; Loss 0.1143033429980278 \n",
      "Step: 6 ; Loss 0.1988280862569809 \n",
      "Step: 0 ; Loss 0.15452764928340912 \n",
      "Step: 3 ; Loss 0.10414472967386246 \n",
      "Step: 6 ; Loss 0.19063538312911987 \n",
      "Step: 0 ; Loss 0.1655033528804779 \n",
      "Step: 3 ; Loss 0.09026219695806503 \n",
      "Step: 6 ; Loss 0.19139732420444489 \n",
      "Step: 0 ; Loss 0.13284844160079956 \n",
      "Step: 3 ; Loss 0.08317945897579193 \n",
      "Step: 6 ; Loss 0.18785709142684937 \n",
      "Step: 0 ; Loss 0.15998511016368866 \n",
      "Step: 3 ; Loss 0.08081719279289246 \n",
      "Step: 6 ; Loss 0.17755335569381714 \n",
      "Step: 0 ; Loss 0.12933409214019775 \n",
      "Step: 3 ; Loss 0.07713698595762253 \n",
      "Step: 6 ; Loss 0.17013944685459137 \n",
      "Step: 0 ; Loss 0.13005562126636505 \n",
      "Step: 3 ; Loss 0.0736517682671547 \n",
      "Step: 6 ; Loss 0.16412290930747986 \n",
      "Step: 0 ; Loss 0.11265050619840622 \n",
      "Step: 3 ; Loss 0.07100538164377213 \n",
      "Step: 6 ; Loss 0.15670788288116455 \n",
      "Step: 0 ; Loss 0.11322981864213943 \n",
      "Step: 3 ; Loss 0.0716286450624466 \n",
      "Step: 6 ; Loss 0.15358223021030426 \n",
      "Step: 0 ; Loss 0.11779201775789261 \n",
      "Step: 3 ; Loss 0.06724488735198975 \n",
      "Step: 6 ; Loss 0.15738476812839508 \n",
      "Step: 0 ; Loss 0.111321821808815 \n",
      "Step: 3 ; Loss 0.06571730971336365 \n",
      "Step: 6 ; Loss 0.14399543404579163 \n",
      "Step: 0 ; Loss 0.10470843315124512 \n",
      "Step: 3 ; Loss 0.06508685648441315 \n",
      "Step: 6 ; Loss 0.14908790588378906 \n",
      "Step: 0 ; Loss 0.10041811317205429 \n",
      "Step: 3 ; Loss 0.0615924671292305 \n",
      "Step: 6 ; Loss 0.14417997002601624 \n",
      "Step: 0 ; Loss 0.09333330392837524 \n",
      "Step: 3 ; Loss 0.06097881495952606 \n",
      "Step: 6 ; Loss 0.14402049779891968 \n",
      "Step: 0 ; Loss 0.10247161984443665 \n",
      "Step: 3 ; Loss 0.05692644789814949 \n",
      "Step: 6 ; Loss 0.14027604460716248 \n",
      "Step: 0 ; Loss 0.09794117510318756 \n",
      "Step: 3 ; Loss 0.05535273626446724 \n",
      "Step: 6 ; Loss 0.13982227444648743 \n",
      "Step: 0 ; Loss 0.09924351423978806 \n",
      "Step: 3 ; Loss 0.053109169006347656 \n",
      "Step: 6 ; Loss 0.13144953548908234 \n",
      "Step: 0 ; Loss 0.09909923374652863 \n",
      "Step: 3 ; Loss 0.060436200350522995 \n",
      "Step: 6 ; Loss 0.1352883130311966 \n",
      "Step: 0 ; Loss 0.07717540115118027 \n",
      "Step: 3 ; Loss 0.051481083035469055 \n",
      "Step: 6 ; Loss 0.13519814610481262 \n",
      "Step: 0 ; Loss 0.08498913794755936 \n",
      "Step: 3 ; Loss 0.05677560716867447 \n",
      "Step: 6 ; Loss 0.12282701581716537 \n",
      "Step: 0 ; Loss 0.07528572529554367 \n",
      "Step: 3 ; Loss 0.04683621972799301 \n",
      "Step: 6 ; Loss 0.12927846610546112 \n",
      "Step: 0 ; Loss 0.07624891400337219 \n",
      "Step: 3 ; Loss 0.04456917569041252 \n",
      "Step: 6 ; Loss 0.12182032316923141 \n",
      "Step: 0 ; Loss 0.07051340490579605 \n",
      "Step: 3 ; Loss 0.04498853534460068 \n",
      "Step: 6 ; Loss 0.11740276962518692 \n",
      "Step: 0 ; Loss 0.06872283667325974 \n",
      "Step: 3 ; Loss 0.043902888894081116 \n",
      "Step: 6 ; Loss 0.19448676705360413 \n",
      "Step: 0 ; Loss 0.16850480437278748 \n",
      "Step: 3 ; Loss 0.06131226569414139 \n",
      "Step: 6 ; Loss 0.12428130209445953 \n",
      "Step: 0 ; Loss 0.07360022515058517 \n",
      "Step: 3 ; Loss 0.07383698970079422 \n",
      "Step: 6 ; Loss 0.2233215570449829 \n",
      "Step: 0 ; Loss 0.12464305758476257 \n",
      "Step: 3 ; Loss 0.32944703102111816 \n",
      "Step: 6 ; Loss 0.23257386684417725 \n",
      "Step: 0 ; Loss 0.05893092602491379 \n",
      "Step: 3 ; Loss 0.19595466554164886 \n",
      "Step: 6 ; Loss 0.20451807975769043 \n",
      "Step: 0 ; Loss 0.22187872231006622 \n",
      "Step: 3 ; Loss 0.1285315603017807 \n",
      "Step: 6 ; Loss 0.3078068792819977 \n",
      "Step: 0 ; Loss 0.27047204971313477 \n",
      "Step: 3 ; Loss 0.11172977834939957 \n",
      "Step: 6 ; Loss 0.14862275123596191 \n",
      "Step: 0 ; Loss 0.08950988203287125 \n",
      "Step: 3 ; Loss 0.06627468019723892 \n",
      "Step: 6 ; Loss 0.11817577481269836 \n",
      "Step: 0 ; Loss 0.1704418659210205 \n",
      "Step: 3 ; Loss 0.07775495201349258 \n",
      "Step: 6 ; Loss 0.14944908022880554 \n",
      "Step: 0 ; Loss 0.19816036522388458 \n",
      "Step: 3 ; Loss 0.06546056270599365 \n",
      "Step: 6 ; Loss 0.16038905084133148 \n",
      "Step: 0 ; Loss 0.08883082866668701 \n",
      "Step: 3 ; Loss 0.0652981773018837 \n",
      "Step: 6 ; Loss 0.12736013531684875 \n",
      "Step: 0 ; Loss 0.08361748605966568 \n",
      "Step: 3 ; Loss 0.07679258286952972 \n",
      "Step: 6 ; Loss 0.20742245018482208 \n",
      "Step: 0 ; Loss 0.07916364073753357 \n",
      "Step: 3 ; Loss 0.0560263954102993 \n",
      "Step: 6 ; Loss 0.12884008884429932 \n",
      "Step: 0 ; Loss 0.06562002003192902 \n",
      "Step: 3 ; Loss 0.05366033688187599 \n",
      "Step: 6 ; Loss 0.13271823525428772 \n",
      "Step: 0 ; Loss 0.06785396486520767 \n",
      "Step: 3 ; Loss 0.0472954697906971 \n",
      "Step: 6 ; Loss 0.11489371955394745 \n",
      "Step: 0 ; Loss 0.06894726306200027 \n",
      "Step: 3 ; Loss 0.0456526018679142 \n",
      "Step: 6 ; Loss 0.10220713913440704 \n",
      "Step: 0 ; Loss 0.060009945183992386 \n",
      "Step: 3 ; Loss 0.04130391776561737 \n",
      "Step: 6 ; Loss 0.10145773738622665 \n",
      "Step: 0 ; Loss 0.05439205467700958 \n",
      "Step: 3 ; Loss 0.039191484451293945 \n",
      "Step: 6 ; Loss 0.09616269171237946 \n",
      "Step: 0 ; Loss 0.051386792212724686 \n",
      "Step: 3 ; Loss 0.036327291280031204 \n",
      "Step: 6 ; Loss 0.08677870780229568 \n",
      "Step: 0 ; Loss 0.046194545924663544 \n",
      "Step: 3 ; Loss 0.03142789006233215 \n",
      "Step: 6 ; Loss 0.08397795259952545 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats = run_bootstrapping(classification_experiment, dataset, params.final_task, num_bootstrap_iters=3, input_key=\"attention_dist\", label_key=\"bias_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': [(0.6421503574746055, 0.8592986796739482), 0.7802021833439663],\n",
       " 'accuracy': [(0.6508184523809524, 0.805920493197279), 0.7503543083900226]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
