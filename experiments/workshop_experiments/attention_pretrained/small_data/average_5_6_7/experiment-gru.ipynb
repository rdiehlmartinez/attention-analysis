{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../../../../..\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from src.experiment import AttentionExperiment, ClassificationExperiment\n",
    "from src.dataset import ExperimentDataset\n",
    "from src.params import Params\n",
    "from src.utils.attention_utils import reduce_attention_dist\n",
    "from src.utils.classification_utils import run_bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers = [5, 6, 7]\n",
      "reducer = avg\n",
      "n_components = 40\n",
      "dropout = 0.3\n",
      "hidden_dim = 200\n",
      "attention_units = 512\n"
     ]
    }
   ],
   "source": [
    "params = Params.read_params(\"gru-params.json\")\n",
    "print(\"layers = {}\".format(params.intermediary_task[\"attention\"][\"layers\"]))\n",
    "print(\"reducer = {}\".format(params.intermediary_task[\"attention\"][\"reducer\"]))\n",
    "print(\"n_components = {}\".format(params.intermediary_task[\"attention\"][\"n_components\"]))\n",
    "print(\"dropout = {}\".format(params.final_task[\"dropout\"]))\n",
    "print(\"hidden_dim = {}\".format(params.final_task[\"hidden_dim\"]))\n",
    "print(\"attention_units = {}\".format(params.final_task[\"attention_units\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2020 15:55:56 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "386it [00:00, 3736.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Length: 324 Keys: dict_keys(['pre_ids', 'masks', 'pre_lens', 'post_in_ids', 'post_out_ids', 'pre_tok_label_ids', 'post_tok_label_ids', 'rel_ids', 'pos_ids', 'categories', 'index', 'bias_label'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ExperimentDataset.init_dataset(params.dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2020 15:55:58 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "04/02/2020 15:55:59 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at ./cache/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "04/02/2020 15:55:59 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file ./cache/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmprkakx8s5\n",
      "04/02/2020 15:56:07 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "04/02/2020 15:56:53 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForMultitaskWithFeaturesOnTop not initialized from pretrained model: ['tok_classifier.out.0.weight', 'tok_classifier.out.0.bias', 'tok_classifier.enricher.0.weight', 'tok_classifier.enricher.0.bias', 'cls_classifier.weight', 'cls_classifier.bias']\n",
      "04/02/2020 15:56:53 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMultitaskWithFeaturesOnTop: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "/u/nlp/anaconda/main/anaconda3/envs/bias_classification/lib/python3.6/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "04/02/2020 15:56:54 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at ./cache/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "04/02/2020 15:56:54 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file ./cache/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpgu4yb45r\n",
      "04/02/2020 15:56:58 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiated joint model with pretrained weights.\n",
      "Succesfully loaded in attention experiment!\n"
     ]
    }
   ],
   "source": [
    "attention_dataloader = dataset.return_dataloader() \n",
    "attention_experiment = AttentionExperiment.initialize_attention_experiment(params.intermediary_task, params.dataset, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef8ad76c90f4186bbcc9e5158a79add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "attention_scores = attention_experiment.extract_attention_scores(attention_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced_attention.shape = torch.Size([324, 80, 40])\n"
     ]
    }
   ],
   "source": [
    "lengths = [int(d[\"pre_lens\"].numpy()) for d in dataset]\n",
    "reduced_attention = reduce_attention_dist(attention_scores, params.intermediary_task[\"attention\"], lengths)\n",
    "dataset.add_data(reduced_attention, \"attention_dist\")\n",
    "dataset.shuffle_data()\n",
    "print(\"reduced_attention.shape = {}\".format(reduced_attention.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is where the classification experiment starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a classification experiment that contains useful methods for classifying bias based on the attention distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_experiment = ClassificationExperiment.init_cls_experiment(params.final_task, params.intermediary_task[\"attention\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e9e4addce243b9966ddaa71e87dfa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cross Validation Iteration', max=5.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epochs', max=150.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epochs', max=150.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epochs', max=150.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epochs', max=150.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epochs', max=150.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stats = run_bootstrapping(classification_experiment, dataset, params.final_task, num_bootstrap_iters=5, input_key=\"attention_dist\", label_key=\"bias_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': [(0.520740922164123, 0.5861565543896371), 0.5539473356336944],\n",
       " 'accuracy': [(0.6326530612244897, 0.6428571428571429), 0.636734693877551]}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "layers = [5, 6, 7]\n",
    "reducer = avg\n",
    "\n",
    "20 components, 100 hidden\n",
    "{'auc': [(0.512433371617045, 0.5900536100003714), 0.5563353653149572],\n",
    " 'accuracy': [(0.663265306122449, 0.6734693877551021), 0.6673469387755103]}\n",
    " {'auc': [(0.5379179998745216, 0.5779152811761508), 0.5519700106656629],\n",
    " 'accuracy': [(0.6562500000000001, 0.6656249999999999), 0.6583333333333333]}\n",
    " ... dropout 0.2\n",
    " {'auc': [(0.5275676014806451, 0.6019077942990986), 0.5566148022669762],\n",
    " 'accuracy': [(0.6562500000000001, 0.6770833333333334), 0.66875]}\n",
    " ... dropout 0.3\n",
    " {'auc': [(0.5234184495054062, 0.6349070414287806), 0.5604303908651735],\n",
    " 'accuracy': [(0.6562500000000001, 0.6770833333333334), 0.6666666666666667]}\n",
    " {'auc': [(0.6042748843664176, 0.6616472941415733), 0.6270035244863619],\n",
    " 'accuracy': [(0.6562500000000001, 0.6666666666666667), 0.6604166666666668]}\n",
    " \n",
    " 30 components, 100 hidden\n",
    "{'auc': [(0.5459252850937946, 0.5872123100252247), 0.5620826468239117],\n",
    " 'accuracy': [(0.5918367346938777, 0.6295918367346939), 0.6040816326530613]}\n",
    " \n",
    " 40 components, 100 hidden\n",
    "{'auc': [(0.5467843703137821, 0.6088528138528139), 0.5867294796706561],\n",
    " 'accuracy': [(0.596875, 0.625), 0.61875]}\n",
    " ... dropout 0.3\n",
    " {'auc': [(0.4509459483298493, 0.5785381125009609), 0.5227872219513087],\n",
    " 'accuracy': [(0.5625, 0.6208333333333333), 0.58125]}\n",
    " \n",
    " 20 components, 150 hidden\n",
    "{'auc': [(0.5292090862318954, 0.5894596799758864), 0.5692758142217926],\n",
    " 'accuracy': [(0.5846938775510203, 0.6408163265306123), 0.6142857142857142]}\n",
    " ... dropout 0.3\n",
    " {'auc': [(0.6002935222672066, 0.6301799370220422), 0.6151540710751238],\n",
    " 'accuracy': [(0.6145833333333335, 0.6645833333333334), 0.6333333333333334]}\n",
    " \n",
    "20 components, 120 hidden\n",
    "{'auc': [(0.5083986928104575, 0.5834885620915031), 0.5483169934640523],\n",
    " 'accuracy': [(0.5833333333333334, 0.6020833333333333), 0.5875]}\n",
    " ... dropout 0.3\n",
    "{'auc': [(0.6001158344579397, 0.663335020242915), 0.6240035987404408],\n",
    " 'accuracy': [(0.6145833333333335, 0.6729166666666667), 0.6333333333333334]}\n",
    " {'auc': [(0.5594136433953367, 0.6463586841161212), 0.6213788415161414],\n",
    " 'accuracy': [(0.6562500000000001, 0.6760416666666668), 0.6645833333333334]}\n",
    " {'auc': [(0.5900822110220607, 0.6015406648489355), 0.5954058222479275],\n",
    " 'accuracy': [(0.6540816326530612, 0.6734693877551021), 0.6653061224489797]}\n",
    " {'auc': [(0.520740922164123, 0.5861565543896371), 0.5539473356336944],\n",
    " 'accuracy': [(0.6326530612244897, 0.6428571428571429), 0.636734693877551]}\n",
    "\n",
    "40 components, 200 hidden\n",
    "{'auc': [(0.49963344933933174, 0.6056653368418072), 0.551034151034151],\n",
    " 'accuracy': [(0.5947916666666667, 0.6343750000000001), 0.6166666666666667]}\n",
    " ... dropout 0.3\n",
    " {'auc': [(0.6308514988905406, 0.6627771951995556), 0.6488276861657518],\n",
    " 'accuracy': [(0.653061224489796, 0.6714285714285715), 0.6571428571428571]}\n",
    " ... dropout 0.4\n",
    " {'auc': [(0.6139665126355454, 0.6609614973412667), 0.6448656420972304],\n",
    " 'accuracy': [(0.653061224489796, 0.689795918367347), 0.6612244897959183]}\n",
    " {'auc': [(0.5536035876747951, 0.6226397939865432), 0.5899459948376358],\n",
    " 'accuracy': [(0.6041666666666666, 0.6416666666666665), 0.6124999999999999]}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_experiment.save_model_weights(\"gru-attention.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
