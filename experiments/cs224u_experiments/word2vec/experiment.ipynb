{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment: Word2Vec experiment \n",
    "------\n",
    "concatenates together the last four attention distributions; using only one attention head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../..')\n",
    "import os\n",
    "import torch\n",
    "from bias_classification import prepare_model \n",
    "from src.utils import *\n",
    "from src.experiment import AttentionExperiment, ClassificationExperiment\n",
    "from models.shallow_nn import ShallowClassifier\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "#os.chdir(\"/Users/sabrieyuboglu/Documents/sabri/school/cs_224u/attention_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = \"../../../experiments/bias_classification/word2vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/05/2019 16:46:54 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../../../tasks/bias_classification/results/cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "440it [00:00, 7635.28it/s]\n"
     ]
    }
   ],
   "source": [
    "params = prepare_model.intialize_params(os.path.join(experiment_dir, \"experiment_params.json\"))\n",
    "dataset = prepare_model.initialize_dataset(params.intermediary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pre_ids', 'masks', 'pre_lens', 'post_in_ids', 'post_out_ids', 'pre_tok_label_ids', 'post_tok_label_ids', 'rel_ids', 'pos_ids', 'categories', 'index', 'bias_label'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run to print out the current key names in the dataset\n",
    "dataset.get_key_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "440it [00:00, 79743.07it/s]\n"
     ]
    }
   ],
   "source": [
    "path_label_data = params.intermediary_task['task_specific_params']['target_data']\n",
    "all_bert_toks = prepare_model.get_sample_toks(path_label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = glove2dict(\"../../../tasks/bias_classification/data/word_vectors/glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_to_word_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Detokenizes an input tokenized by BertTokenizer.\n",
    "    @param tokens (list[str])\n",
    "    @returns sent (str) detokenized str\n",
    "    \"\"\"\n",
    "    bert_to_word = {}\n",
    "    word_tokens = []\n",
    "    word_token_idx = 0\n",
    "    for bert_token_idx, bert_token in enumerate(tokens):\n",
    "        bert_to_word[bert_token_idx] = word_token_idx\n",
    "        if bert_token.startswith(\"##\"):\n",
    "            word_tokens[-1] += bert_token.replace(\"##\", \"\")\n",
    "        else:\n",
    "            word_tokens.append(bert_token)\n",
    "            word_token_idx += 1\n",
    "                        \n",
    "    return word_tokens, bert_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias_embeddings(dataset, path, max_len=80):\n",
    "    \"\"\"\n",
    "    Adds embeddings \n",
    "    @\n",
    "    \"\"\"\n",
    "    token2glove = glove #glove2dict(path)\n",
    "    d = len(next(iter(token2glove.values())))\n",
    "    intermediary_labels = dataset.get_val('pre_tok_label_ids')\n",
    "    embeddings = []\n",
    "    for entry in tqdm(dataset):\n",
    "        idx = int(entry[\"index\"])\n",
    "        bert_toks = all_bert_toks[idx]\n",
    "        word_tokens, bert_to_word = bert_to_word_tokens(bert_toks)\n",
    "        \n",
    "        bias_idx = entry['pre_tok_label_ids'].to(dtype=torch.int).flatten().tolist().index(1)\n",
    "        bias_word = word_tokens[bert_to_word[bias_idx]]\n",
    "        \n",
    "        embedding = token2glove.get(bias_word, np.zeros(d))\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "    tensor = torch.tensor(np.stack(embeddings), dtype=torch.float32)  # num_entries, dim\n",
    "    dataset.add_data(tensor, \"glove_embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:00<00:00, 14323.28it/s]\n"
     ]
    }
   ],
   "source": [
    "add_bias_embeddings(dataset, \"../../../tasks/bias_classification/data/word_vectors/glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sent_embeddings(dataset, path, max_len=80):\n",
    "    \"\"\"\n",
    "    Adds embeddings \n",
    "    @\n",
    "    \"\"\"\n",
    "    token2glove = glove #glove2dict(path)\n",
    "    d = len(next(iter(token2glove.values())))\n",
    "    entries = []\n",
    "    for entry in tqdm(dataset):\n",
    "        idx = int(entry[\"index\"])\n",
    "        sent = detokenize(sentence_toks[idx])        \n",
    "        \n",
    "        embeddings = []\n",
    "        for token in word_tokenize(sent):\n",
    "            # give unkown tokens 0 embedding\n",
    "            embedding = token2glove.get(token, np.zeros(d))\n",
    "            embeddings.append(embedding)\n",
    "        \n",
    "        # pad to max_len \n",
    "        if len(embeddings) < max_len:\n",
    "            embeddings.extend([-1 * np.ones(d)] * (max_len - len(embeddings)))\n",
    "        embeddings = embeddings[:max_len]\n",
    "        tensor = np.stack(embeddings)[b] # max_len, dim\n",
    "        entries.append(tensor)\n",
    "    entries_tensor = torch.tensor(np.stack(entries), dtype=torch.float32)\n",
    "    dataset.add_data(entries_tensor, \"glove_embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:00<00:00, 2705.77it/s]\n"
     ]
    }
   ],
   "source": [
    "add_embeddings(dataset, \"../../../tasks/bias_classification/data/word_vectors/glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to reset params \n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reloading classification experiment with new experiments\n",
    "params = prepare_model.intialize_params(\"../../../experiments/bias_classification/word2vec/experiment_params.json\") \n",
    "classification_experiment = prepare_model.initialize_classification_experiment(params.final_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'shallow_nn', 'window_size': 5, 'input_dim': 100, 'hidden_dim': 10, 'output_dim': 1, 'data_split': {'train_split': 0.7, 'eval_split': 0.3, 'test_split': 0}, 'training_params': {'optimizer': 'adam', 'loss': 'bce_with_logits', 'num_epochs': 200, 'batch_size': 32, 'lr': 0.01}}\n"
     ]
    }
   ],
   "source": [
    "print(params.final_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boostrapping procedure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = run_boostrapping(classification_experiment, dataset, params, input_key='glove_embed', label_key='bias_label', threshold=0.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': [(0.8176440250721501, 0.9243149494949495), 0.87503878710997],\n",
       " 'accuracy': [(0.74, 0.8700000000000001), 0.8078250000000001]}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reloading classification experiment with new experiments\n",
    "params = prepare_model.intialize_params(\"experiment_params.json\") \n",
    "classification_experiment = prepare_model.initialize_classification_experiment(params.final_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'shallow_nn', 'window_size': 5, 'input_dim': 11, 'hidden_dim': 3, 'output_dim': 1, 'data_split': {'train_split': 0.7, 'eval_split': 0.3, 'test_split': 0}, 'training_params': {'optimizer': 'adam', 'loss': 'bce_with_logits', 'num_epochs': 200, 'batch_size': 32, 'lr': 0.01}}\n"
     ]
    }
   ],
   "source": [
    "print(params.final_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boostrapping procedure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = run_boostrapping(classification_experiment, dataset, params, input_key='attention_dist', label_key='bias_label', threshold=0.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': [(0.49685111400227683, 0.8255225075881711), 0.7107909485455589],\n",
       " 'accuracy': [(0.5869186046511627, 0.7502906976744186), 0.6730232558139536]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No bootstrapping -- cannot calculate 95% confidence intervals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = params.final_task['data_split']\n",
    "batch_size = params.final_task['training_params']['batch_size']\n",
    "train_dataloader, eval_dataloader, test_dataloader = dataset.split_train_eval_test(**data_split, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "losses, evaluations = classification_experiment.train_model(train_dataloader, eval_dataloader, input_key='glove_embed', label_key='bias_label', threshold=0.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_losses = [average_data(epoch_losses) for epoch_losses in losses]\n",
    "avg_predictions = [average_data(epoch_evaluations) for epoch_evaluations in evaluations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.652036083155665\n"
     ]
    }
   ],
   "source": [
    "min_loss, max_loss, avg_loss = get_statistics(avg_losses, \"loss\")\n",
    "print(max_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_auc, max_auc, avg_auc = get_statistics(avg_predictions, \"auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9033766233766234\n"
     ]
    }
   ],
   "source": [
    "print(max_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8266955266955267\n"
     ]
    }
   ],
   "source": [
    "print(min_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
