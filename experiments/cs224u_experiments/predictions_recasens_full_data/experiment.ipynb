{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment: Uses Recansen's Features to generate weak labels\n",
    "------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append('../..')\n",
    "import torch\n",
    "from bias_classification import prepare_model \n",
    "from src.utils import *\n",
    "from tasks.bias_classification.lib.tagging.features import Featurizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/06/2019 10:47:34 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../../../tasks/bias_classification/results/cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "440it [00:00, 3372.66it/s]\n"
     ]
    }
   ],
   "source": [
    "params = prepare_model.intialize_params(\"experiment_params.json\")\n",
    "dataset = prepare_model.initialize_dataset(params.intermediary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "440it [00:00, 162098.71it/s]\n"
     ]
    }
   ],
   "source": [
    "path_label_data = params.intermediary_task['task_specific_params']['target_data']\n",
    "all_bert_toks = prepare_model.get_sample_toks(path_label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/06/2019 10:47:37 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../../../tasks/bias_classification/results/cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tok2id = prepare_model.get_tok2id(params.intermediary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = Featurizer(tok2id, params=params.intermediary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_to_word_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Detokenizes an input tokenized by BertTokenizer.\n",
    "    @param tokens (list[str])\n",
    "    @returns sent (str) detokenized str\n",
    "    \"\"\"\n",
    "    bert_to_word = {}\n",
    "    word_tokens = []\n",
    "    word_token_idx = 0\n",
    "    for bert_token_idx, bert_token in enumerate(tokens):\n",
    "        bert_to_word[bert_token_idx] = word_token_idx\n",
    "        if bert_token.startswith(\"##\"):\n",
    "            word_tokens[-1] += bert_token.replace(\"##\", \"\")\n",
    "        else:\n",
    "            word_tokens.append(bert_token)\n",
    "            word_token_idx += 1\n",
    "                        \n",
    "    return word_tokens, bert_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias_features(dataset, featurizer):\n",
    "    \"\"\"\n",
    "    Adds embeddings \n",
    "    @\n",
    "    \"\"\"\n",
    "    intermediary_labels = dataset.get_val('pre_tok_label_ids')\n",
    "    embeddings = []\n",
    "    for entry in tqdm(dataset):\n",
    "        idx = int(entry[\"index\"])\n",
    "        bert_toks = all_bert_toks[idx]\n",
    "        word_tokens, bert_to_word = bert_to_word_tokens(bert_toks)\n",
    "        \n",
    "        bias_idx = entry['pre_tok_label_ids'].to(dtype=torch.int).flatten().tolist().index(1)\n",
    "        bias_word = word_tokens[bert_to_word[bias_idx]]\n",
    "        \n",
    "        features = featurizer.features(entry[\"pre_ids\"].tolist(), \n",
    "                                       entry[\"rel_ids\"].tolist(), \n",
    "                                       entry[\"pos_ids\"].tolist())\n",
    "        embeddings.append(features[bias_idx, :])\n",
    "        \n",
    "    tensor = torch.tensor(np.stack(embeddings), dtype=torch.float32)  # num_entries, dim\n",
    "    dataset.add_data(tensor, \"bias_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaad51e1cf54474293cf46732c52c026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=332), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "add_bias_features(dataset, featurizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = params.final_task['data_split']\n",
    "batch_size = params.final_task['training_params']['batch_size']\n",
    "train_dataloader, eval_dataloader, test_dataloader = dataset.split_train_eval_test(**data_split, batch_size=batch_size)\n",
    "classification_experiment = prepare_model.initialize_classification_experiment(params.final_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch training', max=200, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "losses, evaluations = classification_experiment.train_model(train_dataloader, eval_dataloader, input_key='bias_features', label_key='bias_label', threshold=0.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_losses = [average_data(epoch_losses) for epoch_losses in losses]\n",
    "avg_predictions = [average_data(epoch_evaluations) for epoch_evaluations in evaluations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0405435205662045\n"
     ]
    }
   ],
   "source": [
    "min_loss, max_loss, avg_loss = get_statistics(avg_losses, \"loss\")\n",
    "print(min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_auc, max_auc, avg_auc = get_statistics(avg_predictions, \"auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9746608946608948\n"
     ]
    }
   ],
   "source": [
    "print(max_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9423376623376625\n"
     ]
    }
   ],
   "source": [
    "print(min_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Full dataset\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/06/2019 10:48:34 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../../../tasks/bias_classification/results/cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "53803it [00:08, 6594.19it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = params.intermediary_task['task_specific_params']['full_data']\n",
    "full_dataset = prepare_model.initialize_dataset(params.intermediary_task, data_path=data_path, labels_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = full_dataset.get_val('index')\n",
    "intermediary_labels = full_dataset.get_val('pre_tok_label_ids')\n",
    "skip_indices = []\n",
    "bias_indices = []\n",
    "for i,label in enumerate(intermediary_labels):\n",
    "    try:\n",
    "        bias_indices.append(label.to(torch.int).flatten().tolist().index(1))\n",
    "    except:\n",
    "        skip_indices.append(i)\n",
    "full_dataset.remove_indices(skip_indices)\n",
    "full_dataset.add_data(np.asarray(bias_indices), \"bias_idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53803it [00:00, 157634.32it/s]\n"
     ]
    }
   ],
   "source": [
    "all_bert_toks_full_data = prepare_model.get_sample_toks(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/06/2019 10:48:48 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../../../tasks/bias_classification/results/cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tok2id = prepare_model.get_tok2id(params.intermediary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = Featurizer(tok2id, params=params.intermediary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_to_word_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Detokenizes an input tokenized by BertTokenizer.\n",
    "    @param tokens (list[str])\n",
    "    @returns sent (str) detokenized str\n",
    "    \"\"\"\n",
    "    bert_to_word = {}\n",
    "    word_tokens = []\n",
    "    word_token_idx = 0\n",
    "    for bert_token_idx, bert_token in enumerate(tokens):\n",
    "        if bert_token.startswith(\"##\"):\n",
    "            word_tokens[-1] += bert_token.replace(\"##\", \"\")\n",
    "        else:\n",
    "            word_tokens.append(bert_token)\n",
    "        bert_to_word[bert_token_idx] = len(word_tokens) - 1\n",
    "                        \n",
    "    return word_tokens, bert_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias_features(dataset, featurizer):\n",
    "    \"\"\"\n",
    "    Adds embeddings \n",
    "    @\n",
    "    \"\"\"\n",
    "    intermediary_labels = dataset.get_val('pre_tok_label_ids')\n",
    "    embeddings = []\n",
    "    for entry in tqdm(dataset):\n",
    "        idx = int(entry[\"index\"])\n",
    "        bert_toks = all_bert_toks_full_data[idx]\n",
    "        word_tokens, bert_to_word = bert_to_word_tokens(bert_toks)\n",
    "        \n",
    "        bias_idx = entry['bias_idx']\n",
    "        bias_word = word_tokens[bert_to_word[bias_idx]]\n",
    "        features = featurizer.features(entry[\"pre_ids\"].tolist(), \n",
    "                                       entry[\"rel_ids\"].tolist(), \n",
    "                                       entry[\"pos_ids\"].tolist())\n",
    "        embeddings.append(features[bias_idx, :])\n",
    "        \n",
    "    tensor = torch.tensor(np.stack(embeddings), dtype=torch.float32)  # num_entries, dim\n",
    "    dataset.add_data(tensor, \"bias_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e5a815279e4d019ea0cba7f369a468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=52275), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "add_bias_features(full_dataset, featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = full_dataset.return_dataloader(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {\"input_key\":\"bias_features\", \"label_key\":\"\"}\n",
    "predictions, evaluations = classification_experiment.run_inference(dataloader, **keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_array = np.asarray(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset.add_data(predictions_array, \"marta_weak_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pre_ids', 'masks', 'pre_lens', 'post_in_ids', 'post_out_ids', 'pre_tok_label_ids', 'post_tok_label_ids', 'rel_ids', 'pos_ids', 'categories', 'index', 'bias_idx', 'bias_features', 'marta_weak_labels'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset.get_key_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "pickle.dump(full_dataset, open(\"marta_weak_labels.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[10]['marta_weak_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
