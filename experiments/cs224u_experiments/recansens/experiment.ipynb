{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment: Uses Recansen's Features\n",
    "------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../..')\n",
    "import torch\n",
    "from bias_classification import prepare_model \n",
    "from src.utils import *\n",
    "from tasks.bias_classification.lib.tagging.features import Featurizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/21/2020 17:19:44 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../../../tasks/bias_classification/results/cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "440it [00:00, 2326.37it/s]\n"
     ]
    }
   ],
   "source": [
    "params = prepare_model.intialize_params(\"experiment_params.json\")\n",
    "dataset = prepare_model.initialize_dataset(params.intermediary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pre_ids', 'masks', 'pre_lens', 'post_in_ids', 'post_out_ids', 'pre_tok_label_ids', 'post_tok_label_ids', 'rel_ids', 'pos_ids', 'categories', 'index', 'bias_label'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "440it [00:00, 94786.53it/s]\n"
     ]
    }
   ],
   "source": [
    "path_label_data = params.intermediary_task['task_specific_params']['target_data']\n",
    "all_bert_toks = prepare_model.get_sample_toks(path_label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/05/2019 19:23:32 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../../../tasks/bias_classification/results/cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tok2id = prepare_model.get_tok2id(params.intermediary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = Featurizer(tok2id, params=params.intermediary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_to_word_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Detokenizes an input tokenized by BertTokenizer.\n",
    "    @param tokens (list[str])\n",
    "    @returns sent (str) detokenized str\n",
    "    \"\"\"\n",
    "    bert_to_word = {}\n",
    "    word_tokens = []\n",
    "    for bert_token_idx, bert_token in enumerate(tokens):\n",
    "        if bert_token.startswith(\"##\"):\n",
    "            word_tokens[-1] += bert_token.replace(\"##\", \"\")\n",
    "        else:\n",
    "            word_tokens.append(bert_token)\n",
    "        bert_to_word[bert_token_idx] = len(word_tokens) - 1\n",
    "\n",
    "                        \n",
    "    return word_tokens, bert_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['the', 'latvian', 'government', 'illegals'], {0: 0, 1: 1, 2: 2, 3: 3, 4: 3})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\"the\", \"latvian\", \"government\", \"illegal\", \"##s\"]\n",
    "bert_to_word_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias_features(dataset, featurizer):\n",
    "    \"\"\"\n",
    "    Adds embeddings \n",
    "    @\n",
    "    \"\"\"\n",
    "    intermediary_labels = dataset.get_val('pre_tok_label_ids')\n",
    "    embeddings = []\n",
    "    for entry in tqdm(dataset):\n",
    "        idx = int(entry[\"index\"])\n",
    "        bert_toks = all_bert_toks[idx]\n",
    "        word_tokens, bert_to_word = bert_to_word_tokens(bert_toks)\n",
    "        \n",
    "        bias_idx = entry['pre_tok_label_ids'].to(dtype=torch.int).flatten().tolist().index(1)\n",
    "        bias_word = word_tokens[bert_to_word[bias_idx]]\n",
    "        \n",
    "        features = featurizer.features(entry[\"pre_ids\"].tolist(), \n",
    "                                       entry[\"rel_ids\"].tolist(), \n",
    "                                       entry[\"pos_ids\"].tolist())\n",
    "        embeddings.append(features[bias_idx, :])\n",
    "        \n",
    "    tensor = torch.tensor(np.stack(embeddings), dtype=torch.float32)  # num_entries, dim\n",
    "    dataset.add_data(tensor, \"bias_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f526c142e941db9224d9e2d1f75b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=332), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "add_bias_features(dataset, featurizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to reset params \n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reloading classification experiment with new experiments\n",
    "params = prepare_model.intialize_params(\"experiment_params.json\") \n",
    "classification_experiment = prepare_model.initialize_classification_experiment(params.final_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'shallow_nn', 'window_size': 5, 'input_dim': 90, 'hidden_dim': 10, 'output_dim': 1, 'data_split': {'train_split': 0.7, 'eval_split': 0.3, 'test_split': 0}, 'training_params': {'optimizer': 'adam', 'loss': 'bce_with_logits', 'num_epochs': 200, 'batch_size': 32, 'lr': 0.01}}\n"
     ]
    }
   ],
   "source": [
    "print(params.final_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boostrapping procedure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = run_boostrapping(classification_experiment, dataset, params, input_key='bias_features', label_key='bias_label', threshold=0.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': [(0.9240057679162943, 0.9919756571930486), 0.9621985867990098],\n",
       " 'accuracy': [(0.8747500000000001, 0.9592083333333334), 0.9201166666666668]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reloading classification experiment with new experiments\n",
    "params = prepare_model.intialize_params(\"experiment_params.json\") \n",
    "classification_experiment = prepare_model.initialize_classification_experiment(params.final_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'shallow_nn', 'window_size': 5, 'input_dim': 11, 'hidden_dim': 3, 'output_dim': 1, 'data_split': {'train_split': 0.7, 'eval_split': 0.3, 'test_split': 0}, 'training_params': {'optimizer': 'adam', 'loss': 'bce_with_logits', 'num_epochs': 200, 'batch_size': 32, 'lr': 0.01}}\n"
     ]
    }
   ],
   "source": [
    "print(params.final_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boostrapping procedure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = run_boostrapping(classification_experiment, dataset, params, input_key='bias_features', label_key='bias_label', threshold=0.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': [(0.49685111400227683, 0.8255225075881711), 0.7107909485455589],\n",
       " 'accuracy': [(0.5869186046511627, 0.7502906976744186), 0.6730232558139536]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No bootstrapping -- cannot calculate 95% confidence intervals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{172}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([int(torch.tensor(172, dtype=torch.int16))]) & set([int(torch.tensor(172, dtype=torch.int16))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 19, 20, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 61, 62, 63, 65, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 107, 108, 110, 113, 116, 117, 118, 119, 120, 121, 122, 124, 126, 127, 128, 129, 130, 131, 133, 135, 137, 138, 140, 141, 142, 143, 146, 148, 149, 150, 151, 152, 153, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 171, 172, 173, 174, 175, 177, 178, 179, 180, 183, 185, 187, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 202, 204, 207, 208, 209, 210, 211, 212, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 230, 232, 233, 235, 236, 238, 239, 241, 242, 243, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 278, 279, 280, 281, 282, 284, 285, 286, 288, 290, 292, 293, 294, 295, 296}\n"
     ]
    }
   ],
   "source": [
    "data_split = params.final_task['data_split']\n",
    "batch_size = params.final_task['training_params']['batch_size']\n",
    "train_dataloader, eval_dataloader, test_dataloader = dataset.split_train_eval_test(**data_split, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "losses, evaluations = classification_experiment.train_model(train_dataloader, eval_dataloader, input_key='bias_features', label_key='bias_label', threshold=0.42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_losses = [average_data(epoch_losses) for epoch_losses in losses]\n",
    "avg_predictions = [average_data(epoch_evaluations) for epoch_evaluations in evaluations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04847464928853101\n"
     ]
    }
   ],
   "source": [
    "min_loss, max_loss, avg_loss = get_statistics(avg_losses, \"loss\")\n",
    "print(min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_auc, max_auc, avg_auc = get_statistics(avg_predictions, \"auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9656565656565657\n"
     ]
    }
   ],
   "source": [
    "print(max_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9272150072150072\n"
     ]
    }
   ],
   "source": [
    "print(min_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
