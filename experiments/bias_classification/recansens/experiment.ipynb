{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment: Uses Recansen's Features\n",
    "------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append('../..')\n",
    "import torch\n",
    "from bias_classification import prepare_model \n",
    "from src.utils import *\n",
    "from tasks.bias_classification.lib.tagging.features import Featurizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/05/2019 19:06:57 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../../../tasks/bias_classification/results/cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "440it [00:00, 8016.46it/s]\n"
     ]
    }
   ],
   "source": [
    "params = prepare_model.intialize_params(\"experiment_params.json\")\n",
    "dataset = prepare_model.initialize_dataset(params.intermediary_task)\n",
    "#attention_dataloader = dataset.return_dataloader()\n",
    "#attention_experiment = prepare_model.initialize_attention_experiment(params.intermediary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "440it [00:00, 94786.53it/s]\n"
     ]
    }
   ],
   "source": [
    "path_label_data = params.intermediary_task['task_specific_params']['target_data']\n",
    "all_bert_toks = prepare_model.get_sample_toks(path_label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/05/2019 19:23:32 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../../../tasks/bias_classification/results/cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tok2id = prepare_model.get_tok2id(params.intermediary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = Featurizer(tok2id, params=params.intermediary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pre_ids', 'masks', 'pre_lens', 'post_in_ids', 'post_out_ids', 'pre_tok_label_ids', 'post_tok_label_ids', 'rel_ids', 'pos_ids', 'categories', 'index', 'bias_label'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_key_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pre_ids': tensor([ 1996, 19817,  7361,  5369,  4388,  8945,  8737,  4232,  2003,  2019,\n",
       "          3296,  7069,  3275, 10080,  2971,  2029,  3138,  2173,  1999,  3000,\n",
       "          1010,  2605,  1012,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'masks': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8),\n",
       " 'pre_lens': tensor(23),\n",
       " 'post_in_ids': tensor([ 1945,  1996, 19817,  7361,  5369,  4388,  8945,  8737,  4232,  2003,\n",
       "          2019,  3296,  3275, 10080,  2971,  2029,  3138,  2173,  1999,  3000,\n",
       "          1010,  2605,  1012,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'post_out_ids': tensor([ 1996, 19817,  7361,  5369,  4388,  8945,  8737,  4232,  2003,  2019,\n",
       "          3296,  3275, 10080,  2971,  2029,  3138,  2173,  1999,  3000,  1010,\n",
       "          2605,  1012,  1887,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'pre_tok_label_ids': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2.]),\n",
       " 'post_tok_label_ids': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2.]),\n",
       " 'rel_ids': tensor([ 0,  1,  1,  1,  1,  2,  2,  2,  5,  0,  1,  9,  9,  9,  6,  2, 23, 14,\n",
       "          3,  4,  7, 14,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'pos_ids': tensor([0, 2, 2, 2, 1, 2, 2, 2, 5, 0, 1, 1, 2, 5, 2, 1, 5, 2, 3, 2, 6, 2, 6, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'categories': tensor([0.0054, 0.0296, 0.0065, 0.0040, 0.0232, 0.0117, 0.0117, 0.0374, 0.0187,\n",
       "         0.0209, 0.0342, 0.0157, 0.0176, 0.0393, 0.0296, 0.0205, 0.0415, 0.0104,\n",
       "         0.0118, 0.0306, 0.0376, 0.0336, 0.0335, 0.0202, 0.0311, 0.0198, 0.0135,\n",
       "         0.0401, 0.0263, 0.0399, 0.0264, 0.0008, 0.0148, 0.0344, 0.0398, 0.0010,\n",
       "         0.0306, 0.0232, 0.0386, 0.0196, 0.0345, 0.0051, 0.0153]),\n",
       " 'index': tensor(1, dtype=torch.int16),\n",
       " 'bias_label': tensor(1, dtype=torch.int16)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1996),\n",
       " tensor(19817),\n",
       " tensor(7361),\n",
       " tensor(5369),\n",
       " tensor(4388),\n",
       " tensor(8945),\n",
       " tensor(8737),\n",
       " tensor(4232),\n",
       " tensor(2003),\n",
       " tensor(2019),\n",
       " tensor(3296),\n",
       " tensor(7069),\n",
       " tensor(3275),\n",
       " tensor(10080),\n",
       " tensor(2971),\n",
       " tensor(2029),\n",
       " tensor(3138),\n",
       " tensor(2173),\n",
       " tensor(1999),\n",
       " tensor(3000),\n",
       " tensor(1010),\n",
       " tensor(2605),\n",
       " tensor(1012),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entry[\"pre_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "tensor(1996)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-930bcdd9bf18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeaturizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pre_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rel_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pos_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/sabri/school/cs_224u/attention_analysis/tasks/bias_classification/lib/tagging/features.py\u001b[0m in \u001b[0;36mfeatures\u001b[0;34m(self, id_seq, rel_ids, pos_ids)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mpad_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtoks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2tok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;31m# build list of [word, [tok indices the word came from]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/sabri/school/cs_224u/attention_analysis/tasks/bias_classification/lib/tagging/features.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mpad_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtoks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2tok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;31m# build list of [word, [tok indices the word came from]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: tensor(1996)"
     ]
    }
   ],
   "source": [
    "entry = dataset[0]\n",
    "featurizer.features(entry[\"pre_ids\"], entry[\"rel_ids\"], entry[\"pos_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to reset params \n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reloading classification experiment with new experiments\n",
    "params = prepare_model.intialize_params(\"experiment_params.json\") \n",
    "classification_experiment = prepare_model.initialize_classification_experiment(params.final_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'shallow_nn', 'window_size': 5, 'input_dim': 44, 'hidden_dim': 10, 'output_dim': 1, 'data_split': {'train_split': 0.7, 'eval_split': 0.3, 'test_split': 0}, 'training_params': {'optimizer': 'adam', 'loss': 'bce_with_logits', 'num_epochs': 200, 'batch_size': 32, 'lr': 0.01}}\n"
     ]
    }
   ],
   "source": [
    "print(params.final_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boostrapping procedure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = run_boostrapping(classification_experiment, dataset, params, input_key='attention_dist', label_key='bias_label', threshold=0.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': [(0.6752843679348468, 0.8509239577269152), 0.7677060136452627],\n",
       " 'accuracy': [(0.6395348837209303, 0.8023255813953488), 0.7262790697674417]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reloading classification experiment with new experiments\n",
    "params = prepare_model.intialize_params(\"experiment_params.json\") \n",
    "classification_experiment = prepare_model.initialize_classification_experiment(params.final_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'shallow_nn', 'window_size': 5, 'input_dim': 11, 'hidden_dim': 3, 'output_dim': 1, 'data_split': {'train_split': 0.7, 'eval_split': 0.3, 'test_split': 0}, 'training_params': {'optimizer': 'adam', 'loss': 'bce_with_logits', 'num_epochs': 200, 'batch_size': 32, 'lr': 0.01}}\n"
     ]
    }
   ],
   "source": [
    "print(params.final_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boostrapping procedure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = run_boostrapping(classification_experiment, dataset, params, input_key='attention_dist', label_key='bias_label', threshold=0.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': [(0.49685111400227683, 0.8255225075881711), 0.7107909485455589],\n",
       " 'accuracy': [(0.5869186046511627, 0.7502906976744186), 0.6730232558139536]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No bootstrapping -- cannot calculate 95% confidence intervals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = params.final_task['data_split']\n",
    "batch_size = params.final_task['training_params']['batch_size']\n",
    "train_dataloader, eval_dataloader, test_dataloader = dataset.split_train_eval_test(**data_split, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch training', style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "losses, evaluations = classification_experiment.train_model(train_dataloader, eval_dataloader, input_key='attention_dist', label_key='bias_label', threshold=0.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_losses = [average_data(epoch_losses) for epoch_losses in losses]\n",
    "avg_predictions = [average_data(epoch_evaluations) for epoch_evaluations in evaluations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5602870774269103\n"
     ]
    }
   ],
   "source": [
    "min_loss, max_loss, avg_loss = get_statistics(avg_losses, \"loss\")\n",
    "print(min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_auc, max_auc, avg_auc = get_statistics(avg_predictions, \"auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6637993874070985\n"
     ]
    }
   ],
   "source": [
    "print(max_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6379312454511352\n"
     ]
    }
   ],
   "source": [
    "print(min_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
