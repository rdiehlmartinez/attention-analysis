{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment: Uses Recansen's Features to generate weak labels\n",
    "------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../..')\n",
    "import torch\n",
    "from bias_classification import prepare_model \n",
    "from src.utils import *\n",
    "from tasks.bias_classification.lib.tagging.features import Featurizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/06/2019 10:15:43 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../../../tasks/bias_classification/results/cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "440it [00:00, 6074.72it/s]\n"
     ]
    }
   ],
   "source": [
    "params = prepare_model.intialize_params(\"experiment_params.json\")\n",
    "dataset = prepare_model.initialize_dataset(params.intermediary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "440it [00:00, 140331.06it/s]\n"
     ]
    }
   ],
   "source": [
    "path_label_data = params.intermediary_task['task_specific_params']['target_data']\n",
    "all_bert_toks = prepare_model.get_sample_toks(path_label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/06/2019 10:15:45 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../../../tasks/bias_classification/results/cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tok2id = prepare_model.get_tok2id(params.intermediary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = Featurizer(tok2id, params=params.intermediary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_to_word_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Detokenizes an input tokenized by BertTokenizer.\n",
    "    @param tokens (list[str])\n",
    "    @returns sent (str) detokenized str\n",
    "    \"\"\"\n",
    "    bert_to_word = {}\n",
    "    word_tokens = []\n",
    "    word_token_idx = 0\n",
    "    for bert_token_idx, bert_token in enumerate(tokens):\n",
    "        bert_to_word[bert_token_idx] = word_token_idx\n",
    "        if bert_token.startswith(\"##\"):\n",
    "            word_tokens[-1] += bert_token.replace(\"##\", \"\")\n",
    "        else:\n",
    "            word_tokens.append(bert_token)\n",
    "            word_token_idx += 1\n",
    "                        \n",
    "    return word_tokens, bert_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias_features(dataset, featurizer):\n",
    "    \"\"\"\n",
    "    Adds embeddings \n",
    "    @\n",
    "    \"\"\"\n",
    "    intermediary_labels = dataset.get_val('pre_tok_label_ids')\n",
    "    embeddings = []\n",
    "    for entry in tqdm(dataset):\n",
    "        idx = int(entry[\"index\"])\n",
    "        bert_toks = all_bert_toks[idx]\n",
    "        word_tokens, bert_to_word = bert_to_word_tokens(bert_toks)\n",
    "        \n",
    "        bias_idx = entry['pre_tok_label_ids'].to(dtype=torch.int).flatten().tolist().index(1)\n",
    "        bias_word = word_tokens[bert_to_word[bias_idx]]\n",
    "        \n",
    "        features = featurizer.features(entry[\"pre_ids\"].tolist(), \n",
    "                                       entry[\"rel_ids\"].tolist(), \n",
    "                                       entry[\"pos_ids\"].tolist())\n",
    "        embeddings.append(features[bias_idx, :])\n",
    "        \n",
    "    tensor = torch.tensor(np.stack(embeddings), dtype=torch.float32)  # num_entries, dim\n",
    "    dataset.add_data(tensor, \"bias_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6966569111b44c6a34aef81d1ab40cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=332), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "add_bias_features(dataset, featurizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = params.final_task['data_split']\n",
    "batch_size = params.final_task['training_params']['batch_size']\n",
    "train_dataloader, eval_dataloader, test_dataloader = dataset.split_train_eval_test(**data_split, batch_size=batch_size)\n",
    "classification_experiment = prepare_model.initialize_classification_experiment(params.final_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch training', max=200, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "losses, evaluations = classification_experiment.train_model(train_dataloader, eval_dataloader, input_key='bias_features', label_key='bias_label', threshold=0.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_losses = [average_data(epoch_losses) for epoch_losses in losses]\n",
    "avg_predictions = [average_data(epoch_evaluations) for epoch_evaluations in evaluations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.050349742944898274\n"
     ]
    }
   ],
   "source": [
    "min_loss, max_loss, avg_loss = get_statistics(avg_losses, \"loss\")\n",
    "print(min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_auc, max_auc, avg_auc = get_statistics(avg_predictions, \"auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9708513708513709\n"
     ]
    }
   ],
   "source": [
    "print(max_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9414141414141414\n"
     ]
    }
   ],
   "source": [
    "print(min_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Full dataset\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/06/2019 10:16:10 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../../../tasks/bias_classification/results/cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "53803it [00:08, 6355.95it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = params.intermediary_task['task_specific_params']['full_data']\n",
    "full_dataset = prepare_model.initialize_dataset(params.intermediary_task, data_path=data_path, labels_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = full_dataset.get_val('index')\n",
    "intermediary_labels = full_dataset.get_val('pre_tok_label_ids')\n",
    "skip_indices = []\n",
    "bias_indices = []\n",
    "for i,label in enumerate(intermediary_labels):\n",
    "    try:\n",
    "        bias_indices.append(label.to(torch.int).flatten().tolist().index(1))\n",
    "    except:\n",
    "        skip_indices.append(i)\n",
    "full_dataset.remove_indices(skip_indices)\n",
    "full_dataset.add_data(np.asarray(bias_indices), \"bias_idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53803it [00:00, 161817.26it/s]\n"
     ]
    }
   ],
   "source": [
    "all_bert_toks_full_data = prepare_model.get_sample_toks(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/06/2019 10:17:09 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../../../tasks/bias_classification/results/cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tok2id = prepare_model.get_tok2id(params.intermediary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = Featurizer(tok2id, params=params.intermediary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-49-1a09f5f05f1e>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-49-1a09f5f05f1e>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    word_tokens[-1] += bert_token.replace(\"##\", \"\")\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def bert_to_word_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Detokenizes an input tokenized by BertTokenizer.\n",
    "    @param tokens (list[str])\n",
    "    @returns sent (str) detokenized str\n",
    "    \"\"\"\n",
    "    bert_to_word = {}\n",
    "    word_tokens = []\n",
    "    word_token_idx = 0\n",
    "    for bert_token_idx, bert_token in enumerate(tokens):\n",
    "        if bert_token.startswith(\"##\"):\n",
    "            word_tokens[-1] += bert_token.replace(\"##\", \"\")\n",
    "        else:\n",
    "            word_tokens.append(bert_token)\n",
    "        bert_to_word[bert_token_idx] = len(word_tokens) - 1\n",
    "                        \n",
    "    return word_tokens, bert_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias_features(dataset, featurizer):\n",
    "    \"\"\"\n",
    "    Adds embeddings \n",
    "    @\n",
    "    \"\"\"\n",
    "    intermediary_labels = dataset.get_val('pre_tok_label_ids')\n",
    "    embeddings = []\n",
    "    for entry in tqdm(dataset):\n",
    "        idx = int(entry[\"index\"])\n",
    "        bert_toks = all_bert_toks_full_data[idx]\n",
    "        word_tokens, bert_to_word = bert_to_word_tokens(bert_toks)\n",
    "        \n",
    "        bias_idx = entry['bias_idx']\n",
    "        bias_word = word_tokens[bert_to_word[bias_idx]]\n",
    "        features = featurizer.features(entry[\"pre_ids\"].tolist(), \n",
    "                                       entry[\"rel_ids\"].tolist(), \n",
    "                                       entry[\"pos_ids\"].tolist())\n",
    "        embeddings.append(features[bias_idx, :])\n",
    "        \n",
    "    tensor = torch.tensor(np.stack(embeddings), dtype=torch.float32)  # num_entries, dim\n",
    "    dataset.add_data(tensor, \"bias_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fcc75699024b2c952820c19b702281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=52275), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'latvian', 'government', 'has', 'been', 'criticized', 'for', 'its', 'treatment', 'of', 'illegal', '##s']\n",
      "11\n",
      "11\n",
      "['the', 'latvian', 'government', 'has', 'been', 'criticized', 'for', 'its', 'treatment', 'of', 'illegals']\n",
      "['post', '-', 'trips', 'expansion', '##ism']\n",
      "4\n",
      "4\n",
      "['post', '-', 'trips', 'expansionism']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-159b146b3a31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madd_bias_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-c00313516dbe>\u001b[0m in \u001b[0;36madd_bias_features\u001b[0;34m(dataset, featurizer)\u001b[0m\n\u001b[1;32m     22\u001b[0m         features = featurizer.features(entry[\"pre_ids\"].tolist(), \n\u001b[1;32m     23\u001b[0m                                        \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rel_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                                        entry[\"pos_ids\"].tolist())\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbias_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/juice/scr/scr110/scr/nlp/bert_attention_analysis/tasks/bias_classification/lib/tagging/features.py\u001b[0m in \u001b[0;36mfeatures\u001b[0;34m(self, id_seq, rel_ids, pos_ids)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# get expert featur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mlex_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon_feature_bits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mcontext_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlex_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mexpert_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlex_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/juice/scr/scr110/scr/nlp/bert_attention_analysis/tasks/bias_classification/lib/tagging/features.py\u001b[0m in \u001b[0;36mlexicon_features\u001b[0;34m(self, words, bits)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mbits\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "add_bias_features(full_dataset, featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = full_dataset.return_dataloader(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {\"input_key\":\"bias_features\", \"label_key\":\"\"}\n",
    "predictions, evaluations = classification_experiment.run_inference(dataloader, **keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_array = np.asarray(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset.add_data(predictions_array, \"marta_weak_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pre_ids', 'masks', 'pre_lens', 'post_in_ids', 'post_out_ids', 'pre_tok_label_ids', 'post_tok_label_ids', 'rel_ids', 'pos_ids', 'categories', 'index', 'pos', 'pos_weak_labels'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset.get_key_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "pickle.dump(full_dataset, open(\"marta_weak_labels.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[0]['marta_weak_labels']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
